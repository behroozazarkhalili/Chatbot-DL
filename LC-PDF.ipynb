{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import importlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_packages(package_name: str) -> None:\n",
    "    \"\"\"Installs the packages using pip.\n",
    "    Args:\n",
    "        package_names (str): List of package names to be installed.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # check if the package is already ionstalled and can be imported\n",
    "    if not importlib.util.find_spec(package_name):\n",
    "        try:\n",
    "            # install the package\n",
    "            subprocess.call([\"pip3\", \"install\", package_name])\n",
    "\n",
    "        except Exception as e:\n",
    "            # print the error message\n",
    "            print(f\"the package {package_name} is not installed due to {e}\")\n",
    "    else:\n",
    "        # print that the package is already installed\n",
    "        print(f\"the package {package_name} is already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (0.3.26)\n",
      "Requirement already satisfied: fastapi>=0.85.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (0.97.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (1.10.9)\n",
      "Requirement already satisfied: requests>=2.28 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (4.65.0)\n",
      "Requirement already satisfied: pandas>=1.3 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (1.5.3)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (1.15.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (0.13.2)\n",
      "Requirement already satisfied: hnswlib>=0.7 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (0.7.0)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (0.22.0)\n",
      "Requirement already satisfied: duckdb>=0.7.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (4.5.0)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (0.6.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (7.3.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (3.0.1)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from chromadb) (3.2.0)\n",
      "Requirement already satisfied: lz4 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\n",
      "Requirement already satisfied: certifi in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.5.7)\n",
      "Requirement already satisfied: urllib3>=1.26 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.14)\n",
      "Requirement already satisfied: zstandard in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.21.0)\n",
      "Requirement already satisfied: pytz in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb) (0.27.0)\n",
      "Requirement already satisfied: sympy in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: protobuf in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.1)\n",
      "Requirement already satisfied: coloredlogs in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: packaging in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests>=2.28->chromadb) (2.1.1)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.5.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (3.6.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: langchain in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (0.0.223)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (1.10.9)\n",
      "Requirement already satisfied: langchainplus-sdk<0.0.21,>=0.0.20 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (0.0.20)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (0.5.8)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (2.0.16)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (3.8.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: openai in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: llama-index in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (0.7.1)\n",
      "Requirement already satisfied: numpy in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (1.23.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (4.11.1)\n",
      "Requirement already satisfied: nest-asyncio in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (1.5.6)\n",
      "Requirement already satisfied: typing-inspect==0.8.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (0.8.0)\n",
      "Requirement already satisfied: openai>=0.26.4 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (0.27.8)\n",
      "Requirement already satisfied: urllib3<2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (1.26.14)\n",
      "Requirement already satisfied: dataclasses-json in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (0.5.8)\n",
      "Requirement already satisfied: langchain>=0.0.218 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (0.0.223)\n",
      "Requirement already satisfied: pandas in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (1.5.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (8.2.2)\n",
      "Requirement already satisfied: typing-extensions==4.5.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (4.5.0)\n",
      "Requirement already satisfied: tiktoken in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (2023.6.0)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.15 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from llama-index) (2.0.16)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from typing-inspect==0.8.0->llama-index) (1.0.0)\n",
      "Requirement already satisfied: langchainplus-sdk<0.0.21,>=0.0.20 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain>=0.0.218->llama-index) (0.0.20)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain>=0.0.218->llama-index) (6.0)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain>=0.0.218->llama-index) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain>=0.0.218->llama-index) (1.10.9)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain>=0.0.218->llama-index) (3.8.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain>=0.0.218->llama-index) (2.28.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain>=0.0.218->llama-index) (4.0.2)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from langchain>=0.0.218->llama-index) (2.8.4)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from dataclasses-json->llama-index) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from dataclasses-json->llama-index) (3.19.0)\n",
      "Requirement already satisfied: tqdm in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from openai>=0.26.4->llama-index) (4.65.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from sqlalchemy>=2.0.15->llama-index) (2.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from beautifulsoup4->llama-index) (2.3.2.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from pandas->llama-index) (2022.7.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from tiktoken->llama-index) (2022.10.31)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index) (22.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama-index) (23.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.218->llama-index) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.218->llama-index) (3.4)\n",
      "Requirement already satisfied: python-magic in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (0.4.27)\n",
      "Requirement already satisfied: unstructured in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (0.7.12)\n",
      "Requirement already satisfied: pdfminer.six in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (20221105)\n",
      "Requirement already satisfied: pandas in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (1.5.3)\n",
      "Requirement already satisfied: pillow in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (9.4.0)\n",
      "Requirement already satisfied: xlrd in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (2.0.1)\n",
      "Requirement already satisfied: filetype in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: pypandoc in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (1.11)\n",
      "Requirement already satisfied: pdf2image in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (1.16.3)\n",
      "Requirement already satisfied: python-pptx in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (0.6.21)\n",
      "Requirement already satisfied: python-magic in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: msg-parser in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: requests in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (2.28.2)\n",
      "Requirement already satisfied: tabulate in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: argilla in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (1.11.0)\n",
      "Requirement already satisfied: lxml in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (4.9.2)\n",
      "Requirement already satisfied: openpyxl in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (3.1.2)\n",
      "Requirement already satisfied: nltk in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (3.8.1)\n",
      "Requirement already satisfied: python-docx in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (0.8.11)\n",
      "Requirement already satisfied: chardet in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (5.1.0)\n",
      "Requirement already satisfied: markdown in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from unstructured) (3.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from argilla->unstructured) (23.0)\n",
      "Requirement already satisfied: deprecated~=1.2.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from argilla->unstructured) (1.2.14)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from argilla->unstructured) (4.65.0)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.6.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from argilla->unstructured) (0.9.0)\n",
      "Requirement already satisfied: httpx<0.24,>=0.15 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from argilla->unstructured) (0.23.3)\n",
      "Requirement already satisfied: monotonic in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from argilla->unstructured) (1.6)\n",
      "Requirement already satisfied: backoff in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from argilla->unstructured) (2.2.1)\n",
      "Requirement already satisfied: numpy<1.24.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from argilla->unstructured) (1.23.5)\n",
      "Requirement already satisfied: rich<=13.0.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from argilla->unstructured) (13.0.1)\n",
      "Requirement already satisfied: pydantic>=1.10.7 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from argilla->unstructured) (1.10.9)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.13 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from argilla->unstructured) (1.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from pandas->unstructured) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from pandas->unstructured) (2022.7.1)\n",
      "Requirement already satisfied: olefile>=0.46 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from msg-parser->unstructured) (0.46)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from nltk->unstructured) (2022.10.31)\n",
      "Requirement already satisfied: click in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from nltk->unstructured) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from nltk->unstructured) (1.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from openpyxl->unstructured) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from pdfminer.six->unstructured) (2.1.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from pdfminer.six->unstructured) (41.0.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from python-pptx->unstructured) (3.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests->unstructured) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests->unstructured) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from requests->unstructured) (3.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured) (1.15.1)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from httpx<0.24,>=0.15->argilla->unstructured) (0.16.3)\n",
      "Requirement already satisfied: sniffio in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from httpx<0.24,>=0.15->argilla->unstructured) (1.3.0)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from httpx<0.24,>=0.15->argilla->unstructured) (1.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from pydantic>=1.10.7->argilla->unstructured) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->unstructured) (1.16.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from rich<=13.0.1->argilla->unstructured) (2.14.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from rich<=13.0.1->argilla->unstructured) (0.9.1)\n",
      "Requirement already satisfied: pycparser in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured) (2.21)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured) (3.6.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ermia/anaconda3/envs/clip-retrieval/lib/python3.10/site-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured) (0.14.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg_names = [\"chromadb\", \"langchain\", \"openai\", \"llama-index\", \"python-magic\", \"unstructured\"]\n",
    "# [install_packages(pkg_name) for pkg_name in pkg_names]\n",
    "\n",
    "[subprocess.call([\"pip3\", \"install\", \"-U\", package_name]) for package_name in pkg_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "with open(\"./api_infos/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "openai_api_key = config.get(\"openai_api_key\")\n",
    "print(openai_api_key)\n",
    "\n",
    "hf_api_key = config.get(\"hf_api_key\")\n",
    "print(hf_api_key)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions as emb_funcs\n",
    "import langchain as lc\n",
    "from langchain import vectorstores as lc_vs\n",
    "from langchain import document_loaders as lc_dl\n",
    "from langchain import embeddings as lc_emb\n",
    "from langchain import chat_models as lc_cm\n",
    "from langchain import chains as lc_chains\n",
    "from langchain import agents as lc_agents\n",
    "from langchain import text_splitter as lc_ts\n",
    "from langchain import schema as lc_schema\n",
    "from langchain import prompts as lc_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def run_func(func_name:Callable, *args, **kwargs):\n",
    "    \"\"\"run the function\n",
    "\n",
    "    Args:\n",
    "        func_name (callable): the function to run\n",
    "        *args (tuple): the arguments to pass to the function\n",
    "        **kwargs (dict): the keyword arguments to pass to the function\"\"\"\n",
    "\n",
    "    # check args and kwargs in the functions variables\n",
    "    try :\n",
    "        output = func_name(*args, **kwargs)\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        # print the error message\n",
    "        print(f\"the function {func_name} is not run due to {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text chunk strategy\n",
    "# kw_args = {\"chunk_size\": 512, \"chunk_overlap\": 32, \"separator\": \".\" }\n",
    "kw_args = {\"chunk_size\": 256, \"chunk_overlap\": 32}\n",
    "splitter = run_func(lc_ts.CharacterTextSplitter, **kw_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path:str):\n",
    "    # load the pdf\n",
    "    load_kw_args = {\"file_path\": pdf_path}\n",
    "    loader = run_func(lc_dl.PyMuPDFLoader, **load_kw_args)\n",
    "    pdf_documents = run_func(loader.load)\n",
    "\n",
    "    \n",
    "\n",
    "    # split the documents\n",
    "    split_kw_args = {\"documents\": pdf_documents}\n",
    "    pdf_data_split = run_func(splitter.split_documents, **split_kw_args)\n",
    "\n",
    "    for pdf_data in pdf_data_split:\n",
    "        pdf_data.page_content = pdf_data.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\")\n",
    "\n",
    "    return pdf_data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paths = [\"./pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf\",\n",
    "             \"./pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf\"]\n",
    "\n",
    "pdfs_split = [process_pdf(pdf_path) for pdf_path in pdf_paths]\n",
    "\n",
    "# convert list of liosts to list \n",
    "pdfs_split_data = [item for sublist in pdfs_split for item in sublist]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing Documents in a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.trychroma.com/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593\n",
      "Aurélien Géron Hands-on   Machine Learning    with Scikit-Learn,   Keras & TensorFlow Concepts, Tools, and Techniques   to Build Intelligent Systems TM 2nd Edition Updated for   TensorFlow 2\n",
      "1593\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 0, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "1593\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "final_data = pdfs_split_data\n",
    "\n",
    "texts = [doc.page_content for doc in final_data]\n",
    "metadatas = [doc.metadata for doc in final_data]\n",
    "\n",
    "ids = [str(i) for i in range(len(final_data))]\n",
    "\n",
    "print(len(texts))\n",
    "print(texts[0])\n",
    "\n",
    "print(len(metadatas))\n",
    "print(metadatas[0])\n",
    "\n",
    "print(len(ids))\n",
    "print(ids[0])\n",
    "\n",
    "repo_id = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "task = \"feature-extraction\"\n",
    "kw_args = {\"repo_id\": repo_id, \"task\": task, \"huggingfacehub_api_token\": hf_api_key}\n",
    "hf_emb = run_func(lc_emb.HuggingFaceHubEmbeddings, **kw_args)\n",
    "\n",
    "kw_args = {\"documents\": final_data, \"embedding\": hf_emb, \"ids\": ids, \"collection_name\": \"ml_collection\", \"persist_directory\":\"./data/db/ml-books/\"}\n",
    "\n",
    "final_db = run_func(lc_vs.Chroma.from_documents, **kw_args)\n",
    "final_db.persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload the saved Database and Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://github.com/hwchase17/langchain/issues/3011\n",
    "repo_id = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "task = \"feature-extraction\"\n",
    "kw_args = {\"repo_id\": repo_id, \"task\": task, \"huggingfacehub_api_token\": hf_api_key}\n",
    "hf_emb = run_func(lc_emb.HuggingFaceHubEmbeddings, **kw_args)\n",
    "\n",
    "kw_args = {\"embedding_function\": hf_emb, \"collection_name\": \"ml_collection\", \"persist_directory\":\"./data/db/ml-books/\"}\n",
    "final_db = run_func(lc_vs.Chroma, **kw_args)\n",
    "final_db.persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Language Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 11 345 To get an idea of how those images in MNIST look, let’s visualize examples of the digits 0-9 after reshaping the 784-pixel vectors from our feature matrix into the original 28×28 image that we can plot  via Matplotlib’s imshow function: >>> import matplotlib.pyplot as plt >>> fig, ax = plt.subplots(nrows=2, ncols=5, ...                        sharex=True, sharey=True) >>> ax = ax.flatten() >>> for i in range(10): ...     img = X[y == i][0].reshape(28, 28) ...     ax[i].imshow(img, cmap='Greys') >>> ax[0].set_xticks([]) >>> ax[0].set_yticks([]) >>> plt.tight_layout() >>> plt.show() We should now see a plot of the 2×5 subfigures showing a representative image of each unique digit: Figure 11.4: A plot showing one randomly chosen handwritten digit from each class In addition, let’s also plot multiple examples of the same digit to see how different the handwriting  for each really is: >>> fig, ax = plt.subplots(nrows=5, ...                        ncols=5, ...                        sharex=True, ...                        sharey=True) >>> ax = ax.flatten()\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 373, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Chapter 14 475 >>> mnist_dataset = torchvision.datasets.MNIST( ...     root=image_path, train=True, ...     transform=transform, download=True ... ) >>> from torch.utils.data import Subset >>> mnist_valid_dataset = Subset(mnist_dataset, ...                              torch.arange(10000)) >>> mnist_train_dataset = Subset(mnist_dataset, ...                              torch.arange( ...                                  10000, len(mnist_dataset) ...                              )) >>> mnist_test_dataset = torchvision.datasets.MNIST( ...     root=image_path, train=False, ...     transform=transform, download=False ... ) The MNIST dataset comes with a pre-specified training and test dataset partitioning scheme, but we  also want to create a validation split from the train partition. Hence, we used the first 10,000 training  examples for validation. Note that the images are not sorted by class label, so we do not have to worry  about whether those validation set images are from the same classes. Next, we will construct the data loader with batches of 64 images for the training set and validation  set, respectively: >>> from torch.utils.data import DataLoader >>> batch_size = 64 >>> torch.manual_seed(1) >>> train_dl = DataLoader(mnist_train_dataset, ...                       batch_size, ...                       shuffle=True) >>> valid_dl = DataLoader(mnist_valid_dataset, ...                       batch_size, ...                       shuffle=False) The features we read are of values in the range [0, 1]. Also, we already converted the images to tensors.  The labels are integers from 0 to 9, representing ten digits. Hence, we don’t need to do any scaling or  further conversion. Now, after preparing the dataset, we are ready to implement the CNN we just described.\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 503, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Chapter 13 437 You will recall that in Chapter 12 you learned the way of loading available datasets from the torchvision  module. First, we are going to load the MNIST dataset using the torchvision module. 1.  The setup step includes loading the dataset and specifying hyperparameters (the size of the  train set and test set, and the size of mini-batches): >>> import torchvision >>> from torchvision import transforms >>> image_path = './' >>> transform = transforms.Compose([ ...     transforms.ToTensor() ... ]) >>> mnist_train_dataset = torchvision.datasets.MNIST( ...     root=image_path, train=True, ...     transform=transform, download=False ... ) >>> mnist_test_dataset = torchvision.datasets.MNIST( ...     root=image_path, train=False, ...     transform=transform, download=False ... ) >>> batch_size = 64 >>> torch.manual_seed(1) >>> train_dl = DataLoader(mnist_train_dataset, ...                       batch_size, shuffle=True) Here, we constructed a data loader with batches of 64 samples. Next, we will preprocess the  loaded datasets. 2.  We preprocess the input features and the labels. The features in this project are the pixels  of the images we read from Step 1. We defined a custom transformation using torchvision. transforms.Compose. In this simple case, our transformation consisted only of one method,  ToTensor(). The ToTensor() method converts the pixel features into a floating type tensor  and also normalizes the pixels from the [0, 255] to [0, 1] range. In Chapter 14, Classifying Images with Deep Convolutional Neural Networks, we will see some additional data transformation  methods when we work with more complex image datasets. The labels are integers from 0 to  9 representing ten digits. Hence, we don’t need to do any scaling or further conversion. Note  that we can access the raw pixels using the data attribute, and don’t forget to scale them to  the range [0, 1]. We will construct the model in the next step once the data is preprocessed.\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 465, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "1 By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data. CHAPTER 3 Classification In Chapter 1 I mentioned that the most common supervised learning tasks are regression (predicting values) and classification (predicting classes). In Chapter 2 we explored a regression task, predicting housing values, using various algorithms such as Linear Regression, Decision Trees, and Random Forests (which will be explained in further detail in later chapters). Now we will turn our attention to classification systems. MNIST In this chapter we will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. This set has been studied so much that it is often called the “hello world” of Machine Learning: whenever people come up with a new classification algorithm they are curious to see how it will perform on MNIST, and anyone who learns Machine Learning tackles this dataset sooner or later. Scikit-Learn provides many helper functions to download popular datasets. MNIST is one of them. The following code fetches the MNIST dataset:1 >>> from sklearn.datasets import fetch_openml >>> mnist = fetch_openml('mnist_784', version=1) >>> mnist.keys() dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details',            'categories', 'url']) 85\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 114, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is MNIST datset?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and anyone interested in Machine Learning should have it in their toolbox. SVMs are particularly well suited for classification of complex smallor medium-sized datasets. This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The model whose decision boundary is represented by the dashed line is so bad that it does not even separate the classes properly. The other two models work perfectly on this training set, but their decision boundaries come so close to the instances that these models will probably not perform as well on new instances. In contrast, the solid line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the two classes but also stays as far away from the closest training instances as possible. You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes. This is called large margin classification. 153\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 182, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "3 More generally, when there are n features, the decision function is an n-dimensional hyperplane, and the decision boundary is an (n – 1)-dimensional hyperplane. SVMs: the bias term will be called b, and the feature weights vector will be called w. No bias feature will be added to the input feature vectors. Decision Function and Predictions The linear SVM classifier model predicts the class of a new instance x by simply computing the decision function w⊺ x + b = w1 x1 + ⋯ + wn xn + b. If the result is positive, the predicted class ŷ is the positive class (1), and otherwise it is the negative class (0); see Equation 5-2. Equation 5-2. Linear SVM classifier prediction y = 0 if w⊺x + b < 0, 1 if w⊺x + b ≥ 0 Figure 5-12 shows the decision function that corresponds to the model in the left in Figure 5-4: it is a 2D plane because this dataset has two features (petal width and petal length). The decision boundary is the set of points where the decision function is equal to 0: it is the intersection of two planes, which is a straight line (represented by the thick solid line).3 Figure 5-12. Decision function for the iris dataset Under the Hood  |  165\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 194, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Chapter 3 83 However, one problem with this mapping approach is that the construction of the new features is  computationally very expensive, especially if we are dealing with high-dimensional data. This is where  the so-called kernel trick comes into play. Although we did not go into much detail about how to solve the quadratic programming task to train  an SVM, in practice, we just need to replace the dot product x (i)Tx (j) by 𝜙𝜙𝜙𝜙𝜙(𝑖𝑖)) 𝑇𝑇 𝜙𝜙𝜙𝜙𝜙(𝑗𝑗)) . To save the  expensive step of calculating this dot product between two points explicitly, we define a so-called  kernel function: 𝜅𝜅𝜅𝜅𝜅(𝑖𝑖), 𝜅𝜅(𝑗𝑗)) = 𝜙𝜙𝜅𝜅𝜅(𝑖𝑖)) 𝑇𝑇 𝜙𝜙𝜅𝜅𝜅(𝑗𝑗))  One of the most widely used kernels is the radial basis function (RBF) kernel, which can simply be  called the Gaussian kernel: 𝜅𝜅𝜅𝜅𝜅(𝑖𝑖), 𝜅𝜅(𝑗𝑗)) = exp (− ‖𝜅𝜅(𝑖𝑖) − 𝜅𝜅(𝑗𝑗)‖ 2 2𝜎𝜎2 )  This is often simplified to: 𝜅𝜅𝜅𝜅𝜅(𝑖𝑖), 𝜅𝜅(𝑗𝑗)) = exp (− 𝛾𝛾𝛾𝜅𝜅(𝑖𝑖) − 𝜅𝜅(𝑗𝑗)𝛾 2)  Here, 𝛾𝛾 𝛾 1 2𝜎𝜎2  is a free parameter to be optimized. Roughly speaking, the term “kernel” can be interpreted as a similarity function between a pair of examples. The minus sign inverts the distance measure into a similarity score, and, due to the exponential  term, the resulting similarity score will fall into a range between 1 (for exactly similar examples) and  0 (for very dissimilar examples). Now that we have covered the big picture behind the kernel trick, let’s see if we can train a kernel  SVM that is able to draw a nonlinear decision boundary that separates the XOR data well. Here, we  simply use the SVC class from scikit-learn that we imported earlier and replace the kernel='linear'  parameter with kernel='rbf': >>> svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0) >>> svm.fit(X_xor, y_xor) >>> plot_decision_regions(X_xor, y_xor, classifier=svm) >>> plt.legend(loc='upper left') >>> plt.tight_layout() >>> plt.show()\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 111, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "APPENDIX C SVM Dual Problem To understand duality, you first need to understand the Lagrange multipliers method. The general idea is to transform a constrained optimization objective into an unconstrained one, by moving the constraints into the objective function. Let’s look at a simple example. Suppose you want to find the values of x and y that minimize the function f(x, y) = x2 + 2y, subject to an equality constraint: 3x + 2y + 1 = 0. Using the Lagrange multipliers method, we start by defining a new function called the Lagrangian (or Lagrange function): g(x, y, α) = f(x, y) – α(3x + 2y + 1). Each constraint (in this case just one) is subtracted from the original objective, multiplied by a new variable called a Lagrange multiplier. Joseph-Louis Lagrange showed that if x, y  is a solution to the constrained optimization problem, then there must exist an α such that x, y, α  is a stationary point of the Lagrangian (a stationary point is a point where all partial derivatives are equal to zero). In other words, we can compute the partial derivatives of g(x, y, α) with regard to x, y, and α; we can find the points where these derivatives are all equal to zero; and the solutions to the constrained optimization problem (if they exist) must be among these stationary points. In this example the partial derivatives are:  ∂ ∂x g x, y, α = 2x − 3α ∂ ∂y g x, y, α = 2 − 2α ∂ ∂α g x, y, α = − 3x − 2y − 1 When  all  these  partial  derivatives  are  equal  to  0,  we  find  that 2x − 3α = 2 − 2α = −3x − 2y − 1 = 0, from which we can easily find that x = 3 2, y = − 11 4 , and α = 1. This is the only stationary point, and as it respects the constraint, it must be the solution to the constrained optimization problem. 761\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 790, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is SVM?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However, they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a simple linearly separable dataset: on the left, a Decision Tree can split it easily, while on the right, after the dataset is rotated by 45°, the decision boundary looks unnecessarily convoluted. Although both Decision Trees fit the training set perfectly, it is very likely that the model on the right will not generalize well. One way to limit this problem is to use Principal Component Analysis (see Chapter 8), which often results in a better orientation of the training data. Figure 6-7. Sensitivity to training set rotation More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data. For example, if you just remove the widest Iris versicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide) and train a new Decision Tree, you may get the model represented in Figure 6-8. As you can see, it looks very different from the previous Decision Tree (Figure 6-2). Actually, since the training algorithm used by Scikit-Learn is stochastic,6 you may get very different models even on the same training data (unless you set the random_state hyperparameter). Instability  |  185\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 214, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "CHAPTER 6 Decision Trees Like SVMs, Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. They are powerful algorithms, capable of fitting complex datasets. For example, in Chapter 2 you trained a DecisionTreeRegressor model on the California housing dataset, fitting it perfectly (actually, overfitting it). Decision Trees are also the fundamental components of Random Forests (see Chapter 7), which are among the most powerful Machine Learning algorithms available today. In this chapter we will start by discussing how to train, visualize, and make predictions with Decision Trees. Then we will go through the CART training algorithm used by Scikit-Learn, and we will discuss how to regularize trees and use them for regression tasks. Finally, we will discuss some of the limitations of Decision Trees. Training and Visualizing a Decision Tree To understand Decision Trees, let’s build one and take a look at how it makes predictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 175\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 204, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Chapter 3 87 Let’s consider the following example in which we use a decision tree to decide upon an activity on a  particular day: Figure 3.18: An example of a decision tree Based on the features in our training dataset, the decision tree model learns a series of questions to  infer the class labels of the examples. Although Figure 3.18 illustrates the concept of a decision tree  based on categorical variables, the same concept applies if our features are real numbers, like in the  Iris dataset. For example, we could simply define a cut-off value along the sepal width feature axis  and ask a binary question: “Is the sepal width ≥ 2.8 cm?” Using the decision algorithm, we start at the tree root and split the data on the feature that results in  the largest information gain (IG), which will be explained in more detail in the following section. In  an iterative process, we can then repeat this splitting procedure at each child node until the leaves are  pure. This means that the training examples at each node all belong to the same class. In practice, this  can result in a very deep tree with many nodes, which can easily lead to overfitting. Thus, we typically  want to prune the tree by setting a limit for the maximum depth of the tree.\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 115, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Online SVMs                                                                                                            172 Exercises                                                                                                                        174 6. Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  175 Training and Visualizing a Decision Tree                                                                175 Making Predictions                                                                                                     176 Estimating Class Probabilities                                                                                   178 The CART Training Algorithm                                                                                 179 Computational Complexity                                                                                        180 Gini Impurity or Entropy?                                                                                         180 Regularization Hyperparameters                                                                              181 Regression                                                                                                                     183 Instability                                                                                                                      185 Exercises                                                                                                                        186 7. Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  189 Voting Classifiers                                                                                                         189 Bagging and Pasting                                                                                                    192 Bagging and Pasting in Scikit-Learn                                                                     194 Out-of-Bag Evaluation                                                                                            195 Random Patches and Random Subspaces                                                                196 Random Forests                                                                                                           197 Extra-Trees                                                                                                                198 Feature Importance                                                                                                  198 Boosting                                                                                                                        199 AdaBoost                                                                                                                   200 Gradient Boosting                                                                                                    203 Stacking                                                                                                                         208 Exercises                                                                                                                        211 8. Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  213 The Curse of Dimensionality                                                                                     214 Main Approaches for Dimensionality Reduction                                                   215 Projection                                                                                                                  215 Manifold Learning                                                                                                   218 PCA                                                                                                                                219 Preserving the Variance                                                                                          219 Principal Components                                                                                            220 Projecting Down to d Dimensions                                                                        221 Using Scikit-Learn                                                                                                    222 Explained Variance Ratio                                                                                        222 Choosing the Right Number of Dimensions                                                       223 vi  |  Table of Contents\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 7, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the advantage of Decision Tree?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers – Improving Natural Language Processing with Attention Mechanisms 560 In general, there are two types of positional encodings, an absolute one (as shown in the previous  formula) and a relative one. The former will record absolute positions of words and is sensitive to  word shifts in a sentence. That is to say, absolute positional encodings are fixed vectors for each given  position. On the other hand, relative encodings only maintain the relative position of words and are  invariant to sentence shift. Next, let’s look at the layer normalization mechanism, which was first introduced by J. Ba, J.R. Kiros,  and G.E. Hinton in 2016 in the same-named paper Layer Normalization (URL: https://arxiv.org/ abs/1607.06450). While batch normalization, which we will discuss in more detail in Chapter 17, Generative Adversarial Networks for Synthesizing New Data, is a popular choice in computer vision contexts,  layer normalization is the preferred choice in NLP contexts, where sentence lengths can vary. Figure  16.9 illustrates the main differences of layer and batch normalization side by side: Figure 16.9: A comparison of batch and layer normalization While layer normalization is traditionally performed across all elements in a given feature for each  feature independently, the layer normalization used in transformers extends this concept and computes the normalization statistics across all feature values independently for each training example. Since layer normalization computes mean and standard deviation for each training example, it relaxes  minibatch size constraints or dependencies. In contrast to batch normalization, layer normalization  is thus capable of learning from data with small minibatch sizes and varying lengths. However, note  that the original transformer architecture does not have varying-length inputs (sentences are padded  when needed), and unlike RNNs, there is no recurrence in the model. So, how can we then justify  the use of layer normalization over batch normalization? Transformers are usually trained on very  large text corpora, which requires parallel computation; this can be challenging to achieve with batch  normalization, which has a dependency between training examples. Layer normalization has no such  dependency and is thus a more natural choice for transformers.\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 588, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "time-distributed, they have no way of knowing the position of each word (either relative or absolute). Obviously, the relative and absolute word positions are important, so we need to give this information to the Transformer somehow, and positional embeddings are a good way to do this. Let’s look a bit closer at both these novel components of the Transformer architecture, starting with the positional embeddings. Positional embeddings A positional embedding is a dense vector that encodes the position of a word within a sentence: the ith positional embedding is simply added to the word embedding of the ith word in the sentence. These positional embeddings can be learned by the model, but in the paper the authors preferred to use fixed positional embeddings, defined using the sine and cosine functions of different frequencies. The positional embedding matrix P is defined in Equation 16-2 and represented at the bottom of Figure 16-9 (transposed), where Pp,i is the ith component of the embedding for the word located at the pth position in the sentence. Equation 16-2. Sine/cosine positional embeddings Pp, 2i = sin p/100002i/d Pp, 2i + 1 = cos p/100002i/d Figure 16-9. Sine/cosine positional embedding matrix (transposed, top) with a focus on two values of i (bottom) Attention Mechanisms  |  557\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 586, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "22 This is figure 1 from the paper, reproduced with the kind authorization of the authors. Figure 16-8. The Transformer architecture22 Let’s walk through this figure: • The lefthand part is the encoder. Just like earlier, it takes as input a batch of sentences represented as sequences of word IDs (the input shape is [batch size, max input sentence length]), and it encodes each word into a 512-dimensional representation (so the encoder’s output shape is [batch size, max input sentence length, 512]). Note that the top part of the encoder is stacked N times (in the paper, N = 6). Attention Mechanisms  |  555\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 584, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "• The righthand part is the decoder. During training, it takes the target sentence as input (also represented as a sequence of word IDs), shifted one time step to the right (i.e., a start-of-sequence token is inserted at the beginning). It also receives the outputs of the encoder (i.e., the arrows coming from the left side). Note that the top part of the decoder is also stacked N times, and the encoder stack’s final outputs are fed to the decoder at each of these N levels. Just like earlier, the decoder outputs a probability for each possible next word, at each time step (its output shape is [batch size, max output sentence length, vocabulary length]). • During inference, the decoder cannot be fed targets, so we feed it the previously output words (starting with a start-of-sequence token). So the model needs to be called repeatedly, predicting one more word at every round (which is fed to the decoder at the next round, until the end-of-sequence token is output). • Looking more closely, you can see that you are already familiar with most components: there are two embedding layers, 5 × N skip connections, each of them followed by a layer normalization layer, 2 × N “Feed Forward” modules that are composed of two dense layers each (the first one using the ReLU activation function, the second with no activation function), and finally the output layer is a dense layer using the softmax activation function. All of these layers are timedistributed, so each word is treated independently of all the others. But how can we translate a sentence by only looking at one word at a time? Well, that’s where the new components come in: — The encoder’s Multi-Head Attention layer encodes each word’s relationship with every other word in the same sentence, paying more attention to the most relevant ones. For example, the output of this layer for the word “Queen” in the sentence “They welcomed the Queen of the United Kingdom” will depend on all the words in the sentence, but it will probably pay more attention to the words “United” and “Kingdom” than to the words “They” or “welcomed.” This attention mechanism is called self-attention (the sentence is paying attention to itself). We will discuss exactly how it works shortly. The decoder’s Masked Multi-Head Attention layer does the same thing, but each word is only allowed to attend to words located before it. Finally, the decoder’s upper Multi-Head Attention layer is where the decoder pays attention to the words in the input sentence. For example, the decoder will probably pay close attention to the word “Queen” in the input sentence when it is about to output this word’s translation. — The positional embeddings are simply dense vectors (much like word embeddings) that represent the position of a word in the sentence. The nth positional embedding is added to the word embedding of the nth word in each sentence. This gives the model access to each word’s position, which is needed because the Multi-Head Attention layers do not consider the order or the position of the words; they only look at their relationships. Since all the other layers are 556  |  Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 585, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Positional Encoding?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 This is figure 1 from the paper, reproduced with the kind authorization of the authors. Figure 16-8. The Transformer architecture22 Let’s walk through this figure: • The lefthand part is the encoder. Just like earlier, it takes as input a batch of sentences represented as sequences of word IDs (the input shape is [batch size, max input sentence length]), and it encodes each word into a 512-dimensional representation (so the encoder’s output shape is [batch size, max input sentence length, 512]). Note that the top part of the encoder is stacked N times (in the paper, N = 6). Attention Mechanisms  |  555\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 584, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "20 Ashish Vaswani et al., “Attention Is All You Need,” Proceedings of the 31st International Conference on Neural Information Processing Systems (2017): 6000–6010. 21 Since the Transformer uses time-distributed Dense layers, you could argue that it uses 1D convolutional layers with a kernel size of 1. Attention mechanisms are so powerful that you can actually build state-of-the-art models using only attention mechanisms. Attention Is All You Need: The Transformer Architecture In a groundbreaking 2017 paper,20 a team of Google researchers suggested that “Attention Is All You Need.” They managed to create an architecture called the Transformer, which significantly improved the state of the art in NMT without using any recurrent or convolutional layers,21 just attention mechanisms (plus embedding layers, dense layers, normalization layers, and a few other bits and pieces). As an extra bonus, this architecture was also much faster to train and easier to parallelize, so they managed to train it at a fraction of the time and cost of the previous state-of-the-art models. The Transformer architecture is represented in Figure 16-8. 554  |  Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 583, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Chapter 16 553 Although this transformer architecture was originally designed for language translation, it can be  generalized to other tasks such as English constituency parsing, text generation, and text classification.  Later, we will discuss popular language models, such as BERT and GPT, which were derived from this  original transformer architecture. Figure 16.6, which we adapted from the original transformer paper,  illustrates the main architecture and components we will be discussing in this section: Figure 16.6: The original transformer architecture\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 581, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Now let’s look deeper into the heart of the Transformer model: the Multi-Head Attention layer. Multi-Head Attention To understand how a Multi-Head Attention layer works, we must first understand the Scaled Dot-Product Attention layer, which it is based on. Let’s suppose the encoder analyzed the input sentence “They played chess,” and it managed to understand that the word “They” is the subject and the word “played” is the verb, so it encoded this information in the representations of these words. Now suppose the decoder has already translated the subject, and it thinks that it should translate the verb next. For this, it needs to fetch the verb from the input sentence. This is analog to a dictionary lookup: it’s as if the encoder created a dictionary {“subject”: “They”, “verb”: “played”, …} and the decoder wanted to look up the value that corresponds to the key “verb.” However, the model does not have discrete tokens to represent the keys (like “subject” or “verb”); it has vectorized representations of these concepts (which it learned during training), so the key it will use for the lookup (called the query) will not perfectly match any key in the dictionary. The solution is to compute a similarity measure between the query and each key in the dictionary, and then use the softmax function to convert these similarity scores to weights that add up to 1. If the key that represents the verb is by far the most similar to the query, then that key’s weight will be close to 1. Then the model can compute a weighted sum of the corresponding values, so if the weight of the “verb” key is close to 1, then the weighted sum will be very close to the representation of the word “played.” In short, you can think of this whole process as a differentiable dictionary lookup. The similarity measure used by the Transformer is just the dot product, like in Luong attention. In fact, the equation is the same as for Luong attention, except for a scaling factor. The equation is shown in Equation 16-3, in a vectorized form. Equation 16-3. Scaled Dot-Product Attention Attention ���, ���, ��� = softmax ������⊺ dkeys ��� In this equation: • Q is a matrix containing one row per query. Its shape is [nqueries, dkeys], where nqueries is the number of queries and dkeys is the number of dimensions of each query and each key. • K is a matrix containing one row per key. Its shape is [nkeys, dkeys], where nkeys is the number of keys and values. Attention Mechanisms  |  559\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 588, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain the Transformer architecture?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 16 557 We follow the steps of the single head attention calculation to calculate the context vectors as described in the Parameterizing the self-attention mechanism: scaled dot-product attention section. We will  skip the intermediate steps for brevity and assume that we have computed the context vectors for  the second input element as the query and the eight different attention heads, which we represent as  multihead_z_2 via random data: >>> multihead_z_2 = torch.rand(8, 16) Note that the first dimension indexes over the eight attention heads, and the context vectors, similar to  the input sentences, are 16-dimensional vectors. If this appears complicated, think of multihead_z_2  as eight copies of the 𝑧𝑧(2)  shown in Figure 16.5; that is, we have one 𝑧𝑧(2)  for each of the eight attention  heads. Then, we concatenate these vectors into one long vector of length 𝑑𝑑𝑣𝑣 × ℎ  and use a linear projection (via  a fully connected layer) to map it back to a vector of length 𝑑𝑑𝑣𝑣 . This process is illustrated in Figure 16.7: Figure 16.7: Concatenating the scaled dot-product attention vectors into one vector and passing it  through a linear projection In code, we can implement the concatenation and squashing as follows: >>> linear = torch.nn.Linear(8*16, 16) >>> context_vector_2 = linear(multihead_z_2.flatten()) >>> context_vector_2.shape torch.Size([16])\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 585, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "23 This is the right part of figure 2 from the paper, reproduced with the kind authorization of the authors. Figure 16-10. Multi-Head Attention layer architecture23 As you can see, it is just a bunch of Scaled Dot-Product Attention layers, each preceded by a linear transformation of the values, keys, and queries (i.e., a timedistributed Dense layer with no activation function). All the outputs are simply concatenated, and they go through a final linear transformation (again, timedistributed). But why? What is the intuition behind this architecture? Well, consider the word “played” we discussed earlier (in the sentence “They played chess”). The encoder was smart enough to encode the fact that it is a verb. But the word representation also includes its position in the text, thanks to the positional encodings, and it probably includes many other features that are useful for its translation, such as the fact that it is in the past tense. In short, the word representation encodes many different characteristics of the word. If we just used a single Scaled Dot-Product Attention layer, we would only be able to query all of these characteristics in one shot. This is why the Multi-Head Attention layer applies multiple different linear transformations of the values, keys, and queries: this allows the model to apply many different projections of the word representation into different subspaces, each focusing on a subset of the word’s characteristics. Perhaps one of the linear layers will project the word representation into a subspace where all that remains is the information that the word is a 562  |  Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 591, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Chapter 16 555 To illustrate the multi-head self-attention stack in code, first consider how we created the single  query projection matrix in the previous subsection, Parameterizing the self-attention mechanism: scaled  dot-product attention:  >>> torch.manual_seed(123) >>> d = embedded_sentence.shape[1] >>> one_U_query = torch.rand(d, d) Now, assume we have eight attention heads similar to the original transformer, that is, ℎ = 8 : >>> h = 8 >>> multihead_U_query = torch.rand(h, d, d) >>> multihead_U_key = torch.rand(h, d, d) >>> multihead_U_value = torch.rand(h, d, d) As we can see in the code, multiple attention heads can be added by simply adding an additional  dimension. After initializing the projection matrices, we can compute the projected sequences similar to how  it’s done in scaled dot-product attention. Now, instead of computing one set of query, key, and value  sequences, we need to compute h sets of them. More formally, for example, the computation involving  the query projection for the ith data point in the jth head can be written as follows:  𝒒𝒒𝑗𝑗 (𝑖𝑖) = 𝑼𝑼𝒒𝒒𝑗𝑗𝒙𝒙(𝑖𝑖)  We then repeat this computation for all heads 𝑗𝑗𝑗 𝑗𝑗𝑗𝑗𝑗 𝑗 𝑗 𝑗 𝑗𝑗 . In code, this looks like the following for the second input word as the query: >>> multihead_query_2 = multihead_U_query.matmul(x_2) >>> multihead_query_2.shape torch.Size([8, 16]) The multihead_query_2 matrix has eight rows, where each row corresponds to the jth attention head. Splitting data across multiple attention heads In practice, rather than having a separate matrix for each attention head, transformer  implementations use a single matrix for all attention heads. The attention heads are then  organized into logically separate regions in this matrix, which can be accessed via Boolean  masks. This makes it possible to implement multi-head attention more efficiently because  multiple matrix multiplications can be implemented as a single matrix multiplication  instead. However, for simplicity, we are omitting this implementation detail in this section.\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 583, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Transformers – Improving Natural Language Processing with Attention Mechanisms 558 To summarize, multi-head self-attention is repeating the scaled dot-product attention computation  multiple times in parallel and combining the results. It works very well in practice because the multiple  heads help the model to capture information from different parts of the input, which is very similar to  how the multiple kernels produce multiple channels in a convolutional network, where each channel  can capture different feature information. Lastly, while multi-head attention sounds computationally  expensive, note that the computation can all be done in parallel because there are no dependencies  between the multiple heads. Learning a language model: decoder and masked multi-head  attention Similar to the encoder, the decoder also contains several repeated layers. Besides the two sublayers  that we have already introduced in the previous encoder section (the multi-head self-attention layer  and fully connected layer), each repeated layer also contains a masked multi-head attention sublayer. Masked attention is a variation of the original attention mechanism, where masked attention only  passes a limited input sequence into the model by “masking” out a certain number of words. For  example, if we are building a language translation model with a labeled dataset, at sequence position  i during the training procedure, we only feed in the correct output words from positions 1,…,i-1. All  other words (for instance, those that come after the current position) are hidden from the model to  prevent the model from “cheating.” This is also consistent with the nature of text generation: although  the true translated words are known during training, we know nothing about the ground truth in  practice. Thus, we can only feed the model the solutions to what it has already generated, at position i. Figure 16.8 illustrates how the layers are arranged in the decoder block: Figure 16.8: Layer arrangement in the decoder part First, the previous output words (output embeddings) are passed into the masked multi-head attention layer. Then, the second layer receives both the encoded inputs from the encoder block and the  output of the masked multi-head attention layer into a multi-head attention layer. Finally, we pass the  multi-head attention outputs into a fully connected layer that generates the overall model output: a  probability vector corresponding to the output words.\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 586, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the difference between Scaled Dot Product Attention and Multi-Head Attention?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 6 195 Now, the following confusion matrix plot, with the added labels, should make the results a little bit  easier to interpret: Figure 6.10: A confusion matrix for our data Assuming that class 1 (malignant) is the positive class in this example, our model correctly classified  71 of the examples that belong to class 0 (TN) and 40 examples that belong to class 1 (TP), respectively.  However, our model also incorrectly misclassified two examples from class 1 as class 0 (FN), and it  predicted that one example is malignant although it is a benign tumor (FP). In the next subsection,  we will learn how we can use this information to calculate various error metrics. Optimizing the precision and recall of a classification model Both the prediction error (ERR) and accuracy (ACC) provide general information about how many  examples are misclassified. The error can be understood as the sum of all false predictions divided  by the number of total predictions, and the accuracy is calculated as the sum of correct predictions  divided by the total number of predictions, respectively: 𝐸𝐸𝐸𝐸𝐸𝐸 = 𝐹𝐹𝐹𝐹 + 𝐹𝐹𝐹𝐹 𝐹𝐹𝐹𝐹 + 𝐹𝐹𝐹𝐹 + 𝑇𝑇𝐹𝐹 + 𝑇𝑇𝐹𝐹  The prediction accuracy can then be calculated directly from the error: 𝐴𝐴𝐴𝐴𝐴𝐴 𝐴 𝑇𝑇𝑇𝑇 + 𝑇𝑇𝑇𝑇 𝐹𝐹𝑇𝑇 + 𝐹𝐹𝑇𝑇 + 𝑇𝑇𝑇𝑇 + 𝑇𝑇𝑇𝑇 𝐴 1 − 𝐸𝐸𝐸𝐸𝐸𝐸  The true positive rate (TPR) and false positive rate (FPR) are performance metrics that are especially  useful for imbalanced class problems: 𝐹𝐹𝐹𝐹𝐹𝐹 𝐹 𝐹𝐹𝐹𝐹 𝑁𝑁 𝐹 𝐹𝐹𝐹𝐹 𝐹𝐹𝐹𝐹 + 𝑇𝑇𝑁𝑁 𝑇𝑇𝐹𝐹𝐹𝐹 𝐹 𝑇𝑇𝐹𝐹 𝐹𝐹 𝐹 𝑇𝑇𝐹𝐹 𝐹𝐹𝑁𝑁 + 𝑇𝑇𝐹𝐹\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 223, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      ">>> from sklearn.metrics import confusion_matrix >>> confusion_matrix(y_train_5, y_train_pred) array([[53057,  1522],        [ 1325,  4096]]) Each row in a confusion matrix represents an actual class, while each column represents a predicted class. The first row of this matrix considers non-5 images (the negative class): 53,057 of them were correctly classified as non-5s (they are called true negatives), while the remaining 1,522 were wrongly classified as 5s (false positives). The second row considers the images of 5s (the positive class): 1,325 were wrongly classified as non-5s (false negatives), while the remaining 4,096 were correctly classified as 5s (true positives). A perfect classifier would have only true positives and true negatives, so its confusion matrix would have nonzero values only on its main diagonal (top left to bottom right): >>> y_train_perfect_predictions = y_train_5  # pretend we reached perfection >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,     0],        [    0,  5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive predictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision precision = TP TP + FP TP is the number of true positives, and FP is the number of false positives. A trivial way to have perfect precision is to make one single positive prediction and ensure it is correct (precision = 1/1 = 100%). But this would not be very useful, since the classifier would ignore all but one positive instance. So precision is typically used along with another metric named recall, also called sensitivity or the true positive rate (TPR): this is the ratio of positive instances that are correctly detected by the classifier (Equation 3-2). Equation 3-2. Recall recall = TP TP + FN FN is, of course, the number of false negatives. If you are confused about the confusion matrix, Figure 3-2 may help. Performance Measures  |  91\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 120, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Learning Best Practices for Model Evaluation and Hyperparameter Tuning 194 A confusion matrix is simply a square matrix that reports the counts of the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions of a classifier, as shown in Figure 6.9: Figure 6.9: The confusion matrix Although these metrics can be easily computed manually by comparing the actual and predicted class  labels, scikit-learn provides a convenient confusion_matrix function that we can use, as follows: >>> from sklearn.metrics import confusion_matrix >>> pipe_svc.fit(X_train, y_train) >>> y_pred = pipe_svc.predict(X_test) >>> confmat = confusion_matrix(y_true=y_test, y_pred=y_pred) >>> print(confmat) [[71  1] [ 2 40]] The array that was returned after executing the code provides us with information about the different  types of error the classifier made on the test dataset. We can map this information onto the confusion  matrix illustration in Figure 6.9 using Matplotlib’s matshow function: >>> fig, ax = plt.subplots(figsize=(2.5, 2.5)) >>> ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3) >>> for i in range(confmat.shape[0]): ...     for j in range(confmat.shape[1]): ...         ax.text(x=j, y=i, s=confmat[i, j], ...                 va='center', ha='center') >>> ax.xaxis.set_ticks_position('bottom') >>> plt.xlabel('Predicted label') >>> plt.ylabel('True label') >>> plt.show()\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 222, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Figure 3-2. An illustrated confusion matrix shows examples of true negatives (top left), false positives (top right), false negatives (lower left), and true positives (lower right) Precision and Recall Scikit-Learn provides several functions to compute classifier metrics, including precision and recall: >>> from sklearn.metrics import precision_score, recall_score >>> precision_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1522) 0.7290850836596654 >>> recall_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1325) 0.7555801512636044 Now your 5-detector does not look as shiny as it did when you looked at its accuracy. When it claims an image represents a 5, it is correct only 72.9% of the time. Moreover, it only detects 75.6% of the 5s. It is often convenient to combine precision and recall into a single metric called the F1 score, in particular if you need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and recall (Equation 3-3). Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if both recall and precision are high. Equation 3-3. F1 F1 = 2 1 precision + 1 recall = 2 × precision × recall precision + recall = TP TP + FN + FP 2 92  |  Chapter 3: Classification\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 121, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Confusion Matrix and how one can caulculate it?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 3-5. Precision versus recall You can see that precision really starts to fall sharply around 80% recall. You will probably want to select a precision/recall trade-off just before that drop—for example, at around 60% recall. But of course, the choice depends on your project. Suppose you decide to aim for 90% precision. You look up the first plot and find that you need to use a threshold of about 8,000. To be more precise you can search for the lowest threshold that gives you at least 90% precision (np.argmax() will give you the first index of the maximum value, which in this case means the first True value): threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)] # ~7816 To make predictions (on the training set for now), instead of calling the classifier’s predict() method, you can run this code: y_train_pred_90 = (y_scores >= threshold_90_precision) Let’s check these predictions’ precision and recall: >>> precision_score(y_train_5, y_train_pred_90) 0.9000380083618396 >>> recall_score(y_train_5, y_train_pred_90) 0.4368197749492714 Great, you have a 90% precision classifier! As you can see, it is fairly easy to create a classifier with virtually any precision you want: just set a high enough threshold, and you’re done. But wait, not so fast. A high-precision classifier is not very useful if its recall is too low! 96  |  Chapter 3: Classification\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 125, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "If someone says, “Let’s reach 99% precision,” you should ask, “At what recall?” The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate (FPR). The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to 1 – the true negative rate (TNR), which is the ratio of negative instances that are correctly classified as negative. The TNR is also called specificity. Hence, the ROC curve plots sensitivity (recall) versus 1 – specificity. To plot the ROC curve, you first use the roc_curve() function to compute the TPR and FPR for various threshold values: from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(y_train_5, y_scores) Then you can plot the FPR against the TPR using Matplotlib. This code produces the plot in Figure 3-6: def plot_roc_curve(fpr, tpr, label=None):     plt.plot(fpr, tpr, linewidth=2, label=label)     plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal     [...] # Add axis labels and grid plot_roc_curve(fpr, tpr) plt.show() Once again there is a trade-off: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). Performance Measures  |  97\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 126, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "from sklearn.metrics import precision_recall_curve precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores) Finally, use Matplotlib to plot precision and recall as functions of the threshold value (Figure 3-4): def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):     plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")     plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")     [...] # highlight the threshold and add the legend, axis label, and grid plot_precision_recall_vs_threshold(precisions, recalls, thresholds) plt.show() Figure 3-4. Precision and recall versus the decision threshold You may wonder why the precision curve is bumpier than the recall curve in Figure 3-4. The reason is that precision may sometimes go down when you raise the threshold (although in general it will go up). To understand why, look back at Figure 3-3 and notice what happens when you start from the central threshold and move it just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%). On the other hand, recall can only go down when the threshold is increased, which explains why its curve looks smooth. Another way to select a good precision/recall trade-off is to plot precision directly against recall, as shown in Figure 3-5 (the same threshold as earlier is highlighted). Performance Measures  |  95\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 124, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Figure 3-2. An illustrated confusion matrix shows examples of true negatives (top left), false positives (top right), false negatives (lower left), and true positives (lower right) Precision and Recall Scikit-Learn provides several functions to compute classifier metrics, including precision and recall: >>> from sklearn.metrics import precision_score, recall_score >>> precision_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1522) 0.7290850836596654 >>> recall_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1325) 0.7555801512636044 Now your 5-detector does not look as shiny as it did when you looked at its accuracy. When it claims an image represents a 5, it is correct only 72.9% of the time. Moreover, it only detects 75.6% of the 5s. It is often convenient to combine precision and recall into a single metric called the F1 score, in particular if you need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and recall (Equation 3-3). Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if both recall and precision are high. Equation 3-3. F1 F1 = 2 1 precision + 1 recall = 2 × precision × recall precision + recall = TP TP + FN + FP 2 92  |  Chapter 3: Classification\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 121, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is precision and recall?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be used to train many other models, as we will see. 7 A quadratic equation is of the form y = ax2 + bx + c. Let’s compare the algorithms we’ve discussed so far for Linear Regression6 (recall that m is the number of training instances and n is the number of features); see Table 4-1. Table 4-1. Comparison of algorithms for Linear Regression Algorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn Normal Equation Fast No Slow 0 No N/A SVD Fast No Slow 0 No LinearRegression Batch GD Slow No Fast 2 Yes SGDRegressor Stochastic GD Fast Yes Fast ≥2 Yes SGDRegressor Mini-batch GD Fast Yes Fast ≥2 Yes SGDRegressor There is almost no difference after training: all these algorithms end up with very similar models and make predictions in exactly the same way. Polynomial Regression What if your data is more complex than a straight line? Surprisingly, you can use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression. Let’s look at an example. First, let’s generate some nonlinear data, based on a simple quadratic equation7 (plus some noise; see Figure 4-12): m = 100 X = 6 * np.random.rand(m, 1) 3 y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1) 128  |  Chapter 4: Training Models\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 157, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "from sklearn.linear_model import SGDRegressor sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1) sgd_reg.fit(X, y.ravel()) Once again, you find a solution quite close to the one returned by the Normal Equation: >>> sgd_reg.intercept_, sgd_reg.coef_ (array([4.24365286]), array([2.8250878])) Mini-batch Gradient Descent The last Gradient Descent algorithm we will look at is called Mini-batch Gradient Descent. It is simple to understand once you know Batch and Stochastic Gradient Descent: at each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called mini-batches. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs. The algorithm’s progress in parameter space is less erratic than with Stochastic GD, especially with fairly large mini-batches. As a result, Mini-batch GD will end up walking around a bit closer to the minimum than Stochastic GD—but it may be harder for it to escape from local minima (in the case of problems that suffer from local minima, unlike Linear Regression). Figure 4-11 shows the paths taken by the three Gradient Descent algorithms in parameter space during training. They all end up near the minimum, but Batch GD’s path actually stops at the minimum, while both Stochastic GD and Mini-batch GD continue to walk around. However, don’t forget that Batch GD takes a lot of time to take each step, and Stochastic GD and Mini-batch GD would also reach the minimum if you used a good learning schedule. Figure 4-11. Gradient Descent paths in parameter space Gradient Descent  |  127\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 156, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "12 The standard deviation is generally denoted σ (the Greek letter sigma), and it is the square root of the variance, which is the average of the squared deviation from the mean. When a feature has a bell-shaped normal distribution (also called a Gaussian distribution), which is very common, the “68-95-99.7” rule applies: about 68% of the values fall within 1σ of the mean, 95% within 2σ, and 99.7% within 3σ. Figure 2-7. Summary of each numerical attribute The count, mean, min, and max rows are self-explanatory. Note that the null values are ignored (so, for example, the count of total_bedrooms is 20,433, not 20,640). The std row shows the standard deviation, which measures how dispersed the values are.12 The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indicates the value below which a given percentage of observations in a group of observations fall. For example, 25% of the districts have a housing_median_age lower than 18, while 50% are lower than 29 and 75% are lower than 37. These are often called the 25th percentile (or first quartile), the median, and the 75th percentile (or third quartile). Another quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis). You can either plot this one attribute at a time, or you can call the hist() method on the whole dataset (as shown in the following code example), and it will plot a histogram for each numerical attribute (see Figure 2-8): %matplotlib inline   # only in a Jupyter notebook import matplotlib.pyplot as plt housing.hist(bins=50, figsize=(20,15)) plt.show() Get the Data  |  49\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 78, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Figure 4-1. Randomly generated linear dataset Now let’s compute θ using the Normal Equation. We will use the inv() function from NumPy’s linear algebra module (np.linalg) to compute the inverse of a matrix, and the dot() method for matrix multiplication: X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) The function that we used to generate the data is y = 4 + 3x1 + Gaussian noise. Let’s see what the equation found: >>> theta_best array([[4.21509616],        [2.77011339]]) We would have hoped for θ0 = 4 and θ1 = 3 instead of θ0 = 4.215 and θ1 = 2.770. Close enough, but the noise made it impossible to recover the exact parameters of the original function. Now we can make predictions using θ: >>> X_new = np.array([[0], [2]]) >>> X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance >>> y_predict = X_new_b.dot(theta_best) >>> y_predict array([[4.21509616],        [9.75532293]]) Linear Regression  |  115\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 144, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is normal equation?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Continuous Target Variables with Regression Analysis 284 For instance, we can see that the overall result looks identical to our GD implementation: Figure 9.8: A linear regression plot using scikit-learn Analytical solutions of linear regression As an alternative to using machine learning libraries, there is also a closed-form solution  for solving OLS involving a system of linear equations that can be found in most introductory statistics textbooks: 𝑤𝑤 𝑤 𝑤𝑤𝑤𝑇𝑇𝑤𝑤𝑋−1𝑤𝑤𝑇𝑇𝑦𝑦  We can implement it in Python as follows: # adding a column vector of \"ones\" >>> Xb = np.hstack((np.ones((X.shape[0], 1)), X)) >>> w = np.zeros(X.shape[1]) >>> z = np.linalg.inv(np.dot(Xb.T, Xb)) >>> w = np.dot(z, np.dot(Xb.T, y)) >>> print(f'Slope: {w[1]:.3f}') Slope: 111.666 >>> print(f'Intercept: {w[0]:.3f}') Intercept: 13342.979\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 312, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "from sklearn.linear_model import SGDRegressor sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1) sgd_reg.fit(X, y.ravel()) Once again, you find a solution quite close to the one returned by the Normal Equation: >>> sgd_reg.intercept_, sgd_reg.coef_ (array([4.24365286]), array([2.8250878])) Mini-batch Gradient Descent The last Gradient Descent algorithm we will look at is called Mini-batch Gradient Descent. It is simple to understand once you know Batch and Stochastic Gradient Descent: at each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called mini-batches. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs. The algorithm’s progress in parameter space is less erratic than with Stochastic GD, especially with fairly large mini-batches. As a result, Mini-batch GD will end up walking around a bit closer to the minimum than Stochastic GD—but it may be harder for it to escape from local minima (in the case of problems that suffer from local minima, unlike Linear Regression). Figure 4-11 shows the paths taken by the three Gradient Descent algorithms in parameter space during training. They all end up near the minimum, but Batch GD’s path actually stops at the minimum, while both Stochastic GD and Mini-batch GD continue to walk around. However, don’t forget that Batch GD takes a lot of time to take each step, and Stochastic GD and Mini-batch GD would also reach the minimum if you used a good learning schedule. Figure 4-11. Gradient Descent paths in parameter space Gradient Descent  |  127\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 156, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Chapter 9 281 As you can see in Figure 9.6, the GD algorithm converged approximately after the tenth epoch: Figure 9.6: The loss function versus the number of epochs Next, let’s visualize how well the linear regression line fits the training data. To do so, we will define a  simple helper function that will plot a scatterplot of the training examples and add the regression line: >>> def lin_regplot(X, y, model): ...     plt.scatter(X, y, c='steelblue', edgecolor='white', s=70) ...     plt.plot(X, model.predict(X), color='black', lw=2) Now, we will use this lin_regplot function to plot the living area against the sale price: >>> lin_regplot(X_std, y_std, lr) >>> plt.xlabel(' Living area above ground (standardized)') >>> plt.ylabel('Sale price (standardized)') >>> plt.show()\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 309, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Predicting Continuous Target Variables with Regression Analysis 270 Simple linear regression The goal of simple (univariate) linear regression is to model the relationship between a single feature  (explanatory variable, x) and a continuous-valued target (response variable, y). The equation of a  linear model with one explanatory variable is defined as follows: 𝑦𝑦𝑦 𝑦𝑦𝑦𝑦1𝑥𝑥 𝑥 𝑦𝑥𝑥  Here, the parameter (bias unit), b, represents the y axis intercept and w1 is the weight coefficient of  the explanatory variable. Our goal is to learn the weights of the linear equation to describe the relationship between the explanatory variable and the target variable, which can then be used to predict  the responses of new explanatory variables that were not part of the training dataset. Based on the linear equation that we defined previously, linear regression can be understood as finding  the best-fitting straight line through the training examples, as shown in Figure 9.1: Figure 9.1: A simple one-feature linear regression example This best-fitting line is also called the regression line, and the vertical lines from the regression line  to the training examples are the so-called offsets or residuals—the errors of our prediction.\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 298, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How one can use SGD to solve Linear Regression problem?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Tour of Machine Learning Classifiers Using Scikit-Learn 64 Now, we could use an optimization algorithm such as gradient ascent to maximize this log-likelihood  function. (Gradient ascent works exactly the same way as gradient descent explained in Chapter 2,  except that gradient ascent maximizes a function instead of minimizing it.) Alternatively, let’s rewrite  the log-likelihood as a loss function, L, that can be minimized using gradient descent as in Chapter 2: 𝐿𝐿(𝒘𝒘𝒘 𝒘𝒘) = ∑[−𝑦𝑦(𝑖𝑖) log(𝜎𝜎(𝜎𝜎(𝑖𝑖))) − (1 − 𝑦𝑦(𝑖𝑖)) log(1 − 𝜎𝜎(𝜎𝜎(𝑖𝑖)))] 𝑛𝑛 𝑖𝑖𝑖𝑖   To get a better grasp of this loss function, let’s take a look at the loss that we calculate for one single  training example: 𝐿𝐿(𝜎𝜎(𝑧𝑧) , 𝑦𝑦𝑦 𝑦𝑦, 𝑦𝑦) = −𝑦𝑦 𝑦𝑦𝑦(𝜎𝜎(𝑧𝑧)) − (1 − 𝑦𝑦) 𝑦𝑦𝑦(1 − 𝜎𝜎(𝑧𝑧))  Looking at the equation, we can see that the first term becomes zero if y = 0, and the second term  becomes zero if y = 1: 𝐿𝐿(𝜎𝜎(𝑧𝑧) , 𝑦𝑦𝑦 𝑦𝑦, 𝑦𝑦) = {− log(𝜎𝜎(𝑧𝑧)) if 𝑦𝑦 = 𝑦 − log(𝑦 − 𝜎𝜎(𝑧𝑧)) if 𝑦𝑦 = 𝑦  Deriving the likelihood function We can obtain the expression for the likelihood of the model given the data, ℒ(𝐰𝐰𝐰 𝐰𝐰𝐰|𝐰𝐱𝐱) , as   follows. Given that we have a binary classification problem with class labels 0 and 1, we can   think of the label 1 as a Bernoulli variable—it can take on two values, 0 and 1, with the   probability p of being 1: 𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌𝑌  . For a single data point, we can write this probability  as 𝑃𝑃𝑃𝑃𝑃 𝑃 𝑃𝑃𝑃𝑃𝑃𝑃 𝑃 𝑃(𝑖𝑖𝑖) 𝑃 𝜎𝜎𝑃𝜎𝜎(𝑖𝑖𝑖)  and 𝑃𝑃𝑃𝑃𝑃 𝑃 𝑃𝑃𝑃𝑃𝑃𝑃 𝑃 𝑃(𝑖𝑖𝑖) 𝑃 1 − 𝜎𝜎𝑃𝜎𝜎(𝑖𝑖𝑖) . Putting  these  two  expressions  together,  and  using  the  shorthand  𝑃𝑃𝑃𝑃𝑃 𝑃 𝑃𝑃(𝑖𝑖𝑖 | 𝑋𝑋 𝑃 𝑋(𝑖𝑖𝑖) 𝑃 𝑝𝑝𝑃𝑃𝑃(𝑖𝑖𝑖 | 𝑋(𝑖𝑖𝑖) , we get the probability mass function of the Bernoulli variable: 𝑝𝑝𝑝𝑝𝑝(𝑖𝑖𝑖 | x(𝑖𝑖𝑖) = (𝜎𝜎𝑝𝜎𝜎(𝑖𝑖𝑖)) 𝑦𝑦(𝑖𝑖𝑖 (1 − 𝜎𝜎𝑝𝜎𝜎(𝑖𝑖𝑖)) 1−𝑦𝑦(𝑖𝑖𝑖   We can write the likelihood of the training labels given the assumption that all training  examples are independent, using the multiplication rule to compute the probability that  all events occur, as follows: ℒ(𝐰𝐰𝐰 𝐰𝐰𝐰|𝐰𝐱𝐱) = ∏ 𝑝𝑝𝑝𝑝𝑝(𝑖𝑖)𝐰|𝐰𝐱𝐱(𝑖𝑖); 𝐰𝐰𝐰 𝐰𝐰𝐰 𝑛𝑛 𝑖𝑖𝑖𝑖   Now, substituting the probability mass function of the Bernoulli variable, we arrive at  the expression of the likelihood, which we attempt to maximize by changing the model  parameters: ℒ(𝐰𝐰𝐰 𝐰𝐰𝐰|𝐰𝐱𝐱) = ∏ (𝜎𝜎𝜎𝜎𝜎(𝑖𝑖))) 𝑦𝑦(𝑖𝑖) (1 − 𝜎𝜎𝜎𝜎𝜎(𝑖𝑖))) 1−𝑦𝑦(𝑖𝑖) 𝑛𝑛 𝑖𝑖𝑖1\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 92, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Chapter 11 365 Let’s start with  𝜕𝜕𝜕𝜕 𝜕𝜕𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) , which is the partial derivative of the MSE loss (which simplifies to the squared  error if we omit the mini-batch dimension) with respect to the predicted output score of the first  output node: 𝜕𝜕𝜕𝜕 𝜕𝜕𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) = 𝜕𝜕 𝜕𝜕𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) (𝑦𝑦1 − 𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜)) 2 = 2(𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) − 𝑦𝑦)  The next term is the derivative of the logistic sigmoid activation function that we used in the output layer: 𝜕𝜕𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) 𝜕𝜕𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) = 𝜕𝜕 𝜕𝜕𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) 1 1 + 𝑒𝑒𝑧𝑧1 (𝑜𝑜𝑜𝑜𝑜𝑜) = … = ( 1 1 + 𝑒𝑒𝑧𝑧1 (𝑜𝑜𝑜𝑜𝑜𝑜)) (1 − 1 1 + 𝑒𝑒𝑧𝑧1 (𝑜𝑜𝑜𝑜𝑜𝑜))     = 𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜)(1 − 𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜))   Lastly, we compute the derivative of the net input with respect to the weight: 𝜕𝜕𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) 𝜕𝜕𝜕𝜕1,1 (𝑜𝑜𝑜𝑜𝑜𝑜) = 𝜕𝜕 𝜕𝜕𝜕𝜕1,1 (𝑜𝑜𝑜𝑜𝑜𝑜) 𝑎𝑎1 (ℎ)𝜕𝜕1,1 (𝑜𝑜𝑜𝑜𝑜𝑜) + 𝑏𝑏1 (𝑜𝑜𝑜𝑜𝑜𝑜) = 𝑎𝑎1 (ℎ)  Putting all of it together, we get the following: 𝜕𝜕𝜕𝜕 𝜕𝜕𝜕𝜕1,1 (𝑜𝑜𝑜𝑜𝑜𝑜) = 𝜕𝜕𝜕𝜕 𝜕𝜕𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) ⋅ 𝜕𝜕𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) 𝜕𝜕𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) ⋅ 𝜕𝜕𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) 𝜕𝜕𝜕𝜕1,1 (𝑜𝑜𝑜𝑜𝑜𝑜) = 2(𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜) − 𝑦𝑦𝑦 ⋅ 𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜)(1 − 𝜕𝜕1 (𝑜𝑜𝑜𝑜𝑜𝑜)𝑦 ⋅ 𝜕𝜕1 (ℎ)  We then use this value to update the weight via the familiar stochastic gradient descent update with  a learning rate of 𝜂𝜂 : 𝑤𝑤1,1 (𝑜𝑜𝑜𝑜𝑜𝑜) ≔ 𝑤𝑤1,1 (𝑜𝑜𝑜𝑜𝑜𝑜) − 𝜂𝜂 𝜕𝜕𝜕𝜕 𝜕𝜕𝑤𝑤1,1 (𝑜𝑜𝑜𝑜𝑜𝑜)  In our code implementation of NeuralNetMLP(), we implemented the computation  𝜕𝜕𝜕𝜕 𝜕𝜕𝜕𝜕1,1 (𝑜𝑜𝑜𝑜𝑜𝑜)  in vectorized  form in the .backward() method as follows:         # Part 1: dLoss/dOutWeights         ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight         ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet for convenient re-use                  # input/output dim: [n_examples, n_classes]         d_loss__d_a_out = 2.*(a_out y_onehot) / y.shape[0]         # input/output dim: [n_examples, n_classes]         d_a_out__d_z_out = a_out * (1. a_out) # sigmoid derivative         # output dim: [n_examples, n_classes]         delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"delta (rule)                                                        # placeholder\"         # gradient for output weights                  # [n_examples, n_hidden]\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 393, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Chapter 3 71 After fitting the model on the training data, we plotted the decision regions, training examples, and  test examples, as shown in Figure 3.7: Figure 3.7: Decision regions for scikit-learn’s multi-class logistic regression model Algorithms for convex optimization Note that there exist many different algorithms for solving optimization problems. For  minimizing convex loss functions, such as the logistic regression loss, it is recommended  to use more advanced approaches than regular stochastic gradient descent (SGD). In  fact, scikit-learn implements a whole range of such optimization algorithms, which can  be specified via the solver parameter, namely, 'newton-cg', 'lbfgs', 'liblinear',  'sag', and 'saga'. While the logistic regression loss is convex, most optimization algorithms should converge  to the global loss minimum with ease. However, there are certain advantages of using  one algorithm over the other. For example, in previous versions (for instance, v 0.21),  scikit-learn used 'liblinear' as a default, which cannot handle the multinomial loss  and is limited to the OvR scheme for multiclass classification. However, in scikit-learn v  0.22, the default solver was changed to 'lbfgs', which stands for the limited-memory  Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm (https://en.wikipedia.org/ wiki/Limited-memory_BFGS) and is more flexible in this regard.\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 99, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Chapter 3 75 Via the regularization parameter, 𝜆𝜆 , we can then control how closely we fit the training data, while  keeping the weights small. By increasing the value of 𝜆𝜆 , we increase the regularization strength. Please  note that the bias unit, which is essentially an intercept term or negative threshold, as we learned in  Chapter 2, is usually not regularized. The parameter, C, that is implemented for the LogisticRegression class in scikit-learn comes from  a convention in support vector machines, which will be the topic of the next section. The term C is  inversely proportional to the regularization parameter, 𝜆𝜆 . Consequently, decreasing the value of the  inverse regularization parameter, C, means that we are increasing the regularization strength, which  we can visualize by plotting the L2 regularization path for the two weight coefficients: >>> weights, params = [], [] >>> for c in np.arange(-5, 5): ...     lr = LogisticRegression(C=10.**c, ...                             multi_class='ovr') ...     lr.fit(X_train_std, y_train) ...     weights.append(lr.coef_[1]) ...     params.append(10.**c) >>> weights = np.array(weights) >>> plt.plot(params, weights[:, 0], ...          label='Petal length') >>> plt.plot(params, weights[:, 1], linestyle='--', ...          label='Petal width') >>> plt.ylabel('Weight coefficient') >>> plt.xlabel('C') >>> plt.legend(loc='upper left') >>> plt.xscale('log') >>> plt.show() By executing the preceding code, we fitted 10 logistic regression models with different values for the  inverse-regularization parameter, C. For illustration purposes, we only collected the weight coefficients  of class 1 (here, the second class in the dataset: Iris-versicolor) versus all classifiers—remember  that we are using the OvR technique for multiclass classification.\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 103, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How one can use SGD to solve Logsitic Regression problem?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"‐ \", \"\").replace(\"- \", \"\"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Tin Kam Ho, “The Random Subspace Method for Constructing Decision Forests,” IEEE Transactions on Pattern Analysis and Machine Intelligence 20, no. 8 (1998): 832–844. 9 Tin Kam Ho, “Random Decision Forests,” Proceedings of the Third International Conference on Document Analysis and Recognition 1 (1995): 278. 10 The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees. 11 There are a few notable exceptions: splitter is absent (forced to \"random\"), presort is absent (forced to False), max_samples is absent (forced to 1.0), and base_estimator is absent (forced to DecisionTreeClassi fier with the provided hyperparameters).  and max_samples=1.0) but sampling features (by setting bootstrap_features to True and/or max_features to a value smaller than 1.0) is called the Random Subspaces method.8 Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is a RandomForestRegressor class for regression tasks). The following code uses all available CPU cores to train a Random Forest classifier with 500 trees (each limited to maximum 16 nodes): from sklearn.ensemble import RandomForestClassifier rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1) rnd_clf.fit(X_train, y_train) y_pred_rf = rnd_clf.predict(X_test) With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble itself.11 The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features. The algorithm results in greater tree diversity, which (again) trades a higher bias for a lower variance, generally yielding an overall better model. The following BaggingClassifier is roughly equivalent to the previous RandomForestClassifier: Random Forests  |  197\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 226, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "7 Gilles Louppe and Pierre Geurts, “Ensembles on Random Patches,” Lecture Notes in Computer Science 7523 (2012): 346–361. >>> bag_clf = BaggingClassifier( ...     DecisionTreeClassifier(), n_estimators=500, ...     bootstrap=True, n_jobs=-1, oob_score=True) ... >>> bag_clf.fit(X_train, y_train) >>> bag_clf.oob_score_ 0.90133333333333332 According to this oob evaluation, this BaggingClassifier is likely to achieve about 90.1% accuracy on the test set. Let’s verify this: >>> from sklearn.metrics import accuracy_score >>> y_pred = bag_clf.predict(X_test) >>> accuracy_score(y_test, y_pred) 0.91200000000000003 We get 91.2% accuracy on the test set—close enough! The oob decision function for each training instance is also available through the oob_decision_function_ variable. In this case (since the base estimator has a pre dict_proba() method), the decision function returns the class probabilities for each training instance. For example, the oob evaluation estimates that the first training instance has a 68.25% probability of belonging to the positive class (and 31.75% of belonging to the negative class): >>> bag_clf.oob_decision_function_ array([[0.31746032, 0.68253968],        [0.34117647, 0.65882353],        [1.        , 0.        ],        ...        [1.        , 0.        ],        [0.03108808, 0.96891192],        [0.57291667, 0.42708333]]) Random Patches and Random Subspaces The BaggingClassifier class supports sampling the features as well. Sampling is controlled by two hyperparameters: max_features and bootstrap_features. They work the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features. This technique is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the Random Patches method.7 Keeping all training instances (by setting bootstrap=False 196  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 225, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "Chapter 7 229 We only looked at a very simple bagging example in this section. In practice, more complex classification tasks and a dataset’s high dimensionality can easily lead to overfitting in single decision trees,  and this is where the bagging algorithm can really play to its strengths. Finally, we must note that  the bagging algorithm can be an effective approach to reducing the variance of a model. However,  bagging is ineffective in reducing model bias, that is, models that are too simple to capture the trends  in the data well. This is why we want to perform bagging on an ensemble of classifiers with low bias,  for example, unpruned decision trees. Leveraging weak learners via adaptive boosting In this last section about ensemble methods, we will discuss boosting, with a special focus on its most  common implementation: Adaptive Boosting (AdaBoost). In boosting, the ensemble consists of very simple base classifiers, also often referred to as weak  learners, which often only have a slight performance advantage over random guessing—a typical  example of a weak learner is a decision tree stump. The key concept behind boosting is to focus on  training examples that are hard to classify, that is, to let the weak learners subsequently learn from  misclassified training examples to improve the performance of the ensemble. The following subsections will introduce the algorithmic procedure behind the general concept of  boosting and AdaBoost. Lastly, we will use scikit-learn for a practical classification example. How adaptive boosting works In contrast to bagging, the initial formulation of the boosting algorithm uses random subsets of training examples drawn from the training dataset without replacement; the original boosting procedure  can be summarized in the following four key steps: 1.  Draw a random subset (sample) of training examples, d1, without replacement from the training  dataset, D, to train a weak learner, C1. 2.  Draw a second random training subset, d2, without replacement from the training dataset and  add 50 percent of the examples that were previously misclassified to train a weak learner, C2. AdaBoost recognition The original idea behind AdaBoost was formulated by Robert E. Schapire in 1990 in The  Strength of Weak Learnability, Machine Learning, 5(2): 197-227, 1990, URL: http://rob. schapire.net/papers/strengthofweak.pdf. After Robert Schapire and Yoav Freund  presented the AdaBoost algorithm in the Proceedings of the Thirteenth International Conference (ICML 1996), AdaBoost became one of the most widely used ensemble methods in the  years that followed (Experiments with a New Boosting Algorithm by Y. Freund, R. E. Schapire,  and others, ICML, volume 96, 148-156, 1996). In 2003, Freund and Schapire received the  Gödel Prize for their groundbreaking work, which is a prestigious prize for the most outstanding publications in the field of computer science.\n",
      "{'source': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'file_path': './pdfs/Machine-Learning-with-Pytorch-and-Scikit-Learn.pdf', 'page': 257, 'total_pages': 771, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 16.4 (Windows)', 'producer': 'Adobe PDF Library 16.0', 'creationDate': \"D:20220222120156+05'30'\", 'modDate': \"D:20220222123930+05'30'\", 'trapped': ''}\n",
      "\n",
      "\n",
      "12 Pierre Geurts et al., “Extremely Randomized Trees,” Machine Learning 63, no. 1 (2006): 3–42. bag_clf = BaggingClassifier(     DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),     n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1) Extra-Trees When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting (as discussed earlier). It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular Decision Trees do). A forest of such extremely random trees is called an Extremely Randomized Trees ensemble12 (or Extra-Trees for short). Once again, this technique trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests, because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra TreesRegressor class has the same API as the RandomForestRegressor class. It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an ExtraTreesClassifier. Generally, the only way to know is to try both and compare them using cross-validation (tuning the hyperparameters using grid search). Feature Importance Yet another great quality of Random Forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it (see Chapter 6). Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. You can access the result using the feature_importances_ variable. For example, the following code trains a RandomForestClassifier on the iris dataset (introduced in Chapter 4) and outputs each feature’s importance. It seems that the most important features are the petal length (44%) and width (42%), while sepal length and width are rather unimportant in comparison (11% and 2%, respectively): 198  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "{'source': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'file_path': './pdfs/hands-machine-learning-scikit-learn-tensorflow-2nd.pdf', 'page': 227, 'total_pages': 851, 'format': 'PDF 1.6', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': \"Geron, Aurelien;Safari, an O'Reilly Media Company.;\", 'subject': '', 'keywords': '', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creationDate': 'D:20191010140612Z', 'modDate': \"D:20200106000642-05'00'\", 'trapped': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is bagging method and waht is the difference between bagging method and random forests?\"\n",
    "responses = final_db.similarity_search(query, top_k=5)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.page_content.replace(\"\\n\", \" \").replace(\"\\t\", \" \"))\n",
    "    print(response.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_args = {\"model_name\": \"gpt-3.5-turbo\", \"temperature\": 0, \"openai_api_key\": openai_api_key, \"max_tokens\": 256}\n",
    "# kw_args = {\"model_name\": \"gpt-3.5-turbo-16k\", \"temperature\": 1e-10, \"max_tokens\": 256, \"openai_api_key\": openai_api_key}\n",
    "openai_llm = run_func(lc_cm.ChatOpenAI, **kw_args)\n",
    "\n",
    "kw_args = {\"repo_id\": \"google/flan-t5-xl\", \"model_kwargs\":{\"temperature\":1e-10}}\n",
    "hf_llm = run_func(lc.HuggingFaceHub, **kw_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_template = \"\"\"As a Question Answering bot, your goal is to provide accurate and helpful information from the given context.\n",
    "If you don't know the answer, simply state that you don't know.\n",
    "\n",
    "Remember to provide relevant information to assist the user in understanding the answrer of the question.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "qa_prompt = lc_prompts.PromptTemplate(template=qa_template, input_variables=[\"context\", \"question\"])\n",
    "final_qa = lc_chains.RetrievalQA.from_chain_type(llm=openai_llm, chain_type=\"stuff\", retriever=final_db.as_retriever(), chain_type_kwargs={\"prompt\": qa_prompt}, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MNIST dataset is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. It is a widely used dataset in the field of machine learning and is often referred to as the \"hello world\" of machine learning. The dataset is commonly used for tasks such as image classification and is used to evaluate the performance of different classification algorithms.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is MNIST datset?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Support Vector Machine (SVM) is a powerful and versatile Machine Learning model that can perform linear or nonlinear classification, regression, and outlier detection. It is widely used in the field of Machine Learning and is particularly well-suited for classification tasks on complex small or medium-sized datasets. SVMs work by finding a decision boundary that separates different classes of data while maximizing the margin between the decision boundary and the closest training instances. This is known as large margin classification. SVMs can also use a technique called the kernel trick to handle high-dimensional data efficiently.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is SVM?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The advantage of Decision Trees is that they are simple to understand and interpret, easy to use, versatile, and powerful. They provide intuitive classification rules that can even be applied manually if needed. Decision Trees also do not require feature scaling or centering, and they can handle both numerical and categorical data. Additionally, Decision Trees can estimate the probability that an instance belongs to a particular class.\n"
     ]
    }
   ],
   "source": [
    "query = \"hat is the advantage of Decision Tree?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional encoding is a technique used in the Transformer architecture to provide information about the position of words in a sentence. It is important because attention mechanisms in Transformers do not consider the order or position of words. \n",
      "\n",
      "Positional encoding involves adding a dense vector, called a positional embedding, to the word embedding of each word in the sentence. This embedding encodes the position of the word within the sentence. In the original Transformer paper, fixed positional embeddings were used, defined using sine and cosine functions of different frequencies. These embeddings can also be learned by the model.\n",
      "\n",
      "By incorporating positional encoding, the Transformer model is able to capture the relative and absolute positions of words, allowing it to better understand the context and relationships between words in a sentence.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Positional Encoding?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture is a neural network model that was introduced in a groundbreaking 2017 paper titled \"Attention Is All You Need.\" It was designed to improve the state of the art in Neural Machine Translation (NMT) without using recurrent or convolutional layers, but instead relying solely on attention mechanisms.\n",
      "\n",
      "The Transformer architecture consists of an encoder and a decoder. The encoder takes a batch of sentences as input, represented as sequences of word IDs. It encodes each word into a 512-dimensional representation. The encoder is stacked N times, with N typically set to 6 in the paper. The output of the encoder is a tensor with the shape [batch size, max input sentence length, 512].\n",
      "\n",
      "The decoder is responsible for generating the translated output. It also takes a batch of sentences as input, represented as sequences of word IDs. It uses the attention mechanism to focus on relevant parts of the input during the decoding process. The decoder is also stacked N times, similar to the encoder.\n",
      "\n",
      "The attention mechanism in the Transformer is based on the Scaled Dot-Product Attention layer. It computes a similarity measure between a query and each key in a dictionary, and then uses the softmax function to convert these similarity scores into weights. These weights are used to compute a weighted sum of\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain the Transformer architecture?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Scaled Dot Product Attention and Multi-Head Attention are both components of the attention mechanism used in transformer models. \n",
      "\n",
      "Scaled Dot Product Attention calculates the attention weights between a query and a set of key-value pairs. It computes the dot product between the query and each key, scales the result by the square root of the dimension of the query, and applies a softmax function to obtain the attention weights. These weights are then used to compute a weighted sum of the values, resulting in the context vector.\n",
      "\n",
      "On the other hand, Multi-Head Attention is an extension of Scaled Dot Product Attention that applies multiple linear transformations to the queries, keys, and values. It splits the input into multiple heads, each with its own set of linear projections. Each head performs Scaled Dot Product Attention independently, resulting in multiple context vectors. These context vectors are then concatenated and linearly transformed to obtain the final output.\n",
      "\n",
      "The intuition behind Multi-Head Attention is that it allows the model to capture different aspects or characteristics of the input by projecting it into different subspaces. Each head can focus on a subset of the input's features, enabling the model to capture more diverse and nuanced information.\n",
      "\n",
      "In summary, Scaled Dot Product Attention is a single attention mechanism that computes attention weights and context vectors,\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the difference between Scaled Dot Product Attention and Multi-Head Attention?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A confusion matrix is a square matrix that reports the counts of the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions of a classifier. It is a useful tool for evaluating the performance of a classification model.\n",
      "\n",
      "To calculate a confusion matrix, you compare the actual class labels with the predicted class labels. Each row in the matrix represents an actual class, while each column represents a predicted class. The values in the matrix indicate the number of instances that fall into each category.\n",
      "\n",
      "In the given context, the confusion matrix is calculated using the sklearn.metrics.confusion_matrix function. The function takes the true labels (y_true) and the predicted labels (y_pred) as input and returns the confusion matrix as an array.\n",
      "\n",
      "For example, in the code provided:\n",
      "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
      "\n",
      "The variable \"confmat\" stores the confusion matrix, where the rows represent the true labels and the columns represent the predicted labels.\n",
      "\n",
      "Once you have the confusion matrix, you can use it to calculate various performance metrics such as accuracy, precision, recall, and F1 score. These metrics provide information about the model's performance in terms of correct and incorrect predictions.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Confusion Matrix and how one can caulculate it?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision and recall are performance measures used to evaluate the effectiveness of a binary classifier. \n",
      "\n",
      "Precision is the ratio of true positive predictions to the total number of positive predictions made by the classifier. It measures the accuracy of the positive predictions. A high precision indicates that the classifier has a low rate of false positives.\n",
      "\n",
      "Recall, also known as the true positive rate, is the ratio of true positive predictions to the total number of actual positive instances in the dataset. It measures the ability of the classifier to correctly identify positive instances. A high recall indicates that the classifier has a low rate of false negatives.\n",
      "\n",
      "In the context of the given information, precision and recall are used to evaluate the performance of a classifier with different thresholds. The precision/recall trade-off is discussed, highlighting the importance of considering both precision and recall when selecting a threshold for a classifier.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is precision and recall?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Normal Equation is a closed-form solution for finding the optimal parameters (theta) in Linear Regression. It allows us to directly calculate the values of theta that minimize the cost function.\n",
      "\n",
      "The formula for the Normal Equation is:\n",
      "\n",
      "theta = (X^T * X)^-1 * X^T * y\n",
      "\n",
      "Where:\n",
      "- theta is the vector of optimal parameters\n",
      "- X is the matrix of feature values (including the bias term)\n",
      "- y is the vector of target values\n",
      "\n",
      "In this formula, (X^T * X)^-1 represents the inverse of the matrix product of the transpose of X and X, X^T represents the transpose of X, and * represents matrix multiplication.\n",
      "\n",
      "By using the Normal Equation, we can obtain the optimal parameters without the need for iterative optimization algorithms like Gradient Descent.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is normal equation and write its formula?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Stochastic Gradient Descent (SGD) to solve the Linear Regression problem in scikit-learn, you can follow these steps:\n",
      "\n",
      "1. Import the necessary libraries:\n",
      "```python\n",
      "from sklearn.linear_model import SGDRegressor\n",
      "```\n",
      "\n",
      "2. Create an instance of the SGDRegressor class with the desired parameters:\n",
      "```python\n",
      "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
      "```\n",
      "Here, `max_iter` specifies the maximum number of iterations, `tol` is the tolerance for stopping criteria, `penalty` is the regularization term (set to None for no regularization), and `eta0` is the learning rate.\n",
      "\n",
      "3. Fit the model to the training data:\n",
      "```python\n",
      "sgd_reg.fit(X, y.ravel())\n",
      "```\n",
      "Here, `X` is the feature matrix and `y` is the target variable.\n",
      "\n",
      "4. Obtain the intercept and coefficients of the linear regression model:\n",
      "```python\n",
      "sgd_reg.intercept_, sgd_reg.coef_\n",
      "```\n",
      "The `intercept_` attribute gives the intercept term, and the `coef_` attribute gives the coefficients for each feature.\n",
      "\n",
      "Note that the solution obtained using SGD is expected to be\n"
     ]
    }
   ],
   "source": [
    "query = \"How one can use SGD to solve Linear Regression problem in scikit-learn?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In scikit-learn, you can use the SGDClassifier class to solve the Logistic Regression problem using the Stochastic Gradient Descent (SGD) optimization algorithm. The SGDClassifier can directly classify instances into multiple classes without the need for One-vs-Rest (OvR) or One-vs-One (OvO) strategies.\n",
      "\n",
      "To use SGDClassifier for Logistic Regression, you can follow these steps:\n",
      "\n",
      "1. Import the necessary modules:\n",
      "```python\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "```\n",
      "\n",
      "2. Create an instance of the SGDClassifier and specify the loss function as 'log':\n",
      "```python\n",
      "sgd_clf = SGDClassifier(loss='log', random_state=42)\n",
      "```\n",
      "\n",
      "3. If needed, you can scale the input features using StandardScaler:\n",
      "```python\n",
      "scaler = StandardScaler()\n",
      "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
      "```\n",
      "\n",
      "4. Fit the SGDClassifier to the training data:\n",
      "```python\n",
      "sgd_clf.fit(X_train_scaled, y_train)\n",
      "```\n",
      "\n",
      "5. Predict the class labels for new instances:\n",
      "```python\n",
      "sgd_clf.predict([some_digit])\n",
      "```\n",
      "\n",
      "Note that the decision_function() method of the SGDClassifier returns one value per class,\n"
     ]
    }
   ],
   "source": [
    "query = \"How one can use SGD to solve Logsitic Regression problem in scikit-learn?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging, short for bootstrap aggregating, is an ensemble learning method that combines multiple models (often decision trees) trained on different subsets of the training data. Each model is trained independently, and the final prediction is made by aggregating the predictions of all models, typically through majority voting for classification tasks or averaging for regression tasks. Bagging helps reduce variance and improve the overall performance of the model.\n",
      "\n",
      "Random Forests, on the other hand, is a specific implementation of the bagging method that uses decision trees as base estimators. In addition to sampling the training instances, Random Forests also introduce extra randomness by considering only a random subset of features at each node when growing the trees. This feature sampling results in greater diversity among the trees, which further reduces variance and improves the model's performance.\n",
      "\n",
      "In summary, bagging is a general ensemble learning method that combines multiple models, while Random Forests is a specific implementation of bagging that uses decision trees and introduces additional randomness through feature sampling.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is bagging method and waht is the difference between bagging method and random forests?\"\n",
    "response = final_qa.run(query)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the zero-shot-react-description agent will use the \"description\" string to select tools\n",
    "tools = [\n",
    "    lc_agents.Tool(\n",
    "        name = \"QA\",\n",
    "        func=final_qa.run,\n",
    "        description=\"\"\"useful for when a user is interested in asking questions about the context, and the context is a document.\n",
    "                    Input should be a fully formed question.\"\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = lc_agents.initialize_agent(tools, openai_llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to find information about the MNIST dataset.\n",
      "Action: QA\n",
      "Action Input: \"What is the MNIST dataset?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe MNIST dataset is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. It is a widely used dataset in the field of machine learning and is often referred to as the \"hello world\" of machine learning. Many classification algorithms are tested on this dataset to evaluate their performance.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The MNIST dataset is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. It is a widely used dataset in the field of machine learning and is often referred to as the \"hello world\" of machine learning. Many classification algorithms are tested on this dataset to evaluate their performance.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The MNIST dataset is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. It is a widely used dataset in the field of machine learning and is often referred to as the \"hello world\" of machine learning. Many classification algorithms are tested on this dataset to evaluate their performance.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is MNIST datset?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the QA tool to find the answer to this question.\n",
      "Action: QA\n",
      "Action Input: \"What is SVM?\"\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mSVM stands for Support Vector Machine. It is a powerful and versatile machine learning model that can be used for linear or nonlinear classification, regression, and outlier detection. SVMs are particularly well-suited for classification of complex small or medium-sized datasets. The core concept of SVMs is to find a decision boundary that not only separates the classes but also stays as far away from the closest training instances as possible, which is called large margin classification. SVMs can be used for both linear and nonlinear classification tasks. In linear SVM classification, the decision function is a hyperplane, and the decision boundary is an (n-1)-dimensional hyperplane, where n is the number of features. SVMs can also use the kernel trick to handle high-dimensional data efficiently.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: SVM stands for Support Vector Machine. It is a powerful and versatile machine learning model that can be used for linear or nonlinear classification, regression, and outlier detection. SVMs are particularly well-suited for classification of complex small or medium-sized datasets. The core concept of SVMs is to find a decision boundary that not only separates the classes but also stays as far away from the closest training instances as possible, which is called large margin classification. SVMs can be used for both linear and nonlinear classification tasks. In linear SVM classification, the decision function is a hyperplane, and the decision boundary is an (n-1)-dimensional hyperplane, where n is the number of features. SVMs can also use the kernel trick to handle high-dimensional data efficiently.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "SVM stands for Support Vector Machine. It is a powerful and versatile machine learning model that can be used for linear or nonlinear classification, regression, and outlier detection. SVMs are particularly well-suited for classification of complex small or medium-sized datasets. The core concept of SVMs is to find a decision boundary that not only separates the classes but also stays as far away from the closest training instances as possible, which is called large margin classification. SVMs can be used for both linear and nonlinear classification tasks. In linear SVM classification, the decision function is a hyperplane, and the decision boundary is an (n-1)-dimensional hyperplane, where n is the number of features. SVMs can also use the kernel trick to handle high-dimensional data efficiently.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is SVM?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI should use the QA tool to find the advantage of Decision Tree.\n",
      "Action: QA\n",
      "Action Input: \"What is the advantage of Decision Tree?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe advantage of Decision Trees is that they are simple to understand and interpret, easy to use, versatile, and powerful. They can perform both classification and regression tasks, and even multioutput tasks. Decision Trees are capable of fitting complex datasets and are the fundamental components of Random Forests, which are among the most powerful Machine Learning algorithms available today.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The advantage of Decision Trees is that they are simple to understand and interpret, easy to use, versatile, and powerful. They can perform both classification and regression tasks, and even multioutput tasks. Decision Trees are capable of fitting complex datasets and are the fundamental components of Random Forests, which are among the most powerful Machine Learning algorithms available today.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The advantage of Decision Trees is that they are simple to understand and interpret, easy to use, versatile, and powerful. They can perform both classification and regression tasks, and even multioutput tasks. Decision Trees are capable of fitting complex datasets and are the fundamental components of Random Forests, which are among the most powerful Machine Learning algorithms available today.\n"
     ]
    }
   ],
   "source": [
    "query = \"hat is the advantage of Decision Tree?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI should use the QA tool to find the answer to this question.\n",
      "Action: QA\n",
      "Action Input: What is Positional Encoding?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPositional encoding is a technique used in the Transformer architecture to provide information about the position of words in a sentence. It is important because attention mechanisms in Transformers do not consider the order or position of words. \n",
      "\n",
      "Positional encoding involves adding a dense vector, called a positional embedding, to the word embedding of each word in the sentence. This embedding encodes the position of the word within the sentence. In the original Transformer paper, fixed positional embeddings were used, defined using sine and cosine functions of different frequencies. These embeddings can also be learned by the model.\n",
      "\n",
      "By incorporating positional encoding, the Transformer model is able to capture the relative and absolute positions of words, allowing it to better understand the context and relationships between words in a sentence.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: Positional encoding is a technique used in the Transformer architecture to provide information about the position of words in a sentence. It involves adding a positional embedding to the word embedding of each word in the sentence, allowing the model to capture the relative and absolute positions of words.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Positional encoding is a technique used in the Transformer architecture to provide information about the position of words in a sentence. It involves adding a positional embedding to the word embedding of each word in the sentence, allowing the model to capture the relative and absolute positions of words.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Positional Encoding?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI should use QA to find a document that explains the Transformer architecture.\n",
      "Action: QA\n",
      "Action Input: \"Explain the Transformer architecture\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe Transformer architecture is a neural network architecture that was introduced in a groundbreaking 2017 paper titled \"Attention Is All You Need\" by Ashish Vaswani et al. It was originally designed for language translation tasks but can be generalized to other tasks such as text generation, text classification, and English constituency parsing.\n",
      "\n",
      "The main idea behind the Transformer architecture is to replace traditional recurrent or convolutional layers with attention mechanisms. This allows the model to focus on different parts of the input sequence when making predictions, rather than relying on sequential processing.\n",
      "\n",
      "The Transformer architecture consists of an encoder and a decoder. The encoder takes a batch of sentences as input, represented as sequences of word IDs. It encodes each word into a 512-dimensional representation. The encoder is stacked N times, where N is typically set to 6.\n",
      "\n",
      "The decoder is responsible for generating the output sequence. It also takes a batch of sentences as input and encodes each word into a 512-dimensional representation. The decoder is also stacked N times.\n",
      "\n",
      "The heart of the Transformer model is the Multi-Head Attention layer. This layer is based on the Scaled Dot-Product Attention layer. It allows the model to compute a similarity measure between a query and each key in a dictionary, and then use the softmax function to\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The Transformer architecture is a neural network architecture introduced in a 2017 paper titled \"Attention Is All You Need\". It replaces traditional recurrent or convolutional layers with attention mechanisms, allowing the model to focus on different parts of the input sequence. It consists of an encoder and a decoder, both stacked N times. The Multi-Head Attention layer is the key component of the Transformer model.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The Transformer architecture is a neural network architecture introduced in a 2017 paper titled \"Attention Is All You Need\". It replaces traditional recurrent or convolutional layers with attention mechanisms, allowing the model to focus on different parts of the input sequence. It consists of an encoder and a decoder, both stacked N times. The Multi-Head Attention layer is the key component of the Transformer model.\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain the Transformer architecture?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use QA to find the answer to this question.\n",
      "Action: QA\n",
      "Action Input: \"What is the difference between Scaled Dot Product Attention and Multi-Head Attention?\"\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mScaled Dot Product Attention and Multi-Head Attention are both components of the attention mechanism used in transformer models. \n",
      "\n",
      "Scaled Dot Product Attention calculates the attention weights between a query and a set of key-value pairs. It computes the dot product between the query and each key, scales the result by the square root of the dimension of the query, and applies a softmax function to obtain the attention weights. These weights are then used to compute a weighted sum of the values, resulting in the context vector.\n",
      "\n",
      "Multi-Head Attention, on the other hand, applies multiple sets of linear transformations to the queries, keys, and values before performing the Scaled Dot Product Attention. This allows the model to project the word representations into different subspaces, each focusing on a subset of the word's characteristics. The outputs of the multiple attention heads are concatenated and passed through another linear transformation to obtain the final context vector.\n",
      "\n",
      "In summary, Scaled Dot Product Attention is a single attention mechanism that calculates attention weights and context vectors, while Multi-Head Attention applies multiple attention mechanisms in parallel to capture different aspects of the input and combine their results.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Scaled Dot Product Attention is a single attention mechanism that calculates attention weights and context vectors, while Multi-Head Attention applies multiple attention mechanisms in parallel to capture different aspects of the input and combine their results.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Scaled Dot Product Attention is a single attention mechanism that calculates attention weights and context vectors, while Multi-Head Attention applies multiple attention mechanisms in parallel to capture different aspects of the input and combine their results.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the difference between Scaled Dot Product Attention and Multi-Head Attention?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to find information about the Confusion Matrix and how to calculate it.\n",
      "Action: QA\n",
      "Action Input: \"What is Confusion Matrix and how to calculate it?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mA confusion matrix is a table that is used to evaluate the performance of a classification model. It provides a summary of the predictions made by the model on a test dataset, comparing them to the actual labels of the data.\n",
      "\n",
      "The confusion matrix is a square matrix that has rows and columns representing the actual and predicted classes, respectively. Each cell in the matrix represents the count of instances that fall into a particular combination of actual and predicted classes.\n",
      "\n",
      "To calculate a confusion matrix, you can use the `confusion_matrix` function from the `sklearn.metrics` module in Python. This function takes two arguments: the true labels of the data (`y_true`) and the predicted labels (`y_pred`). It returns a 2D array that represents the confusion matrix.\n",
      "\n",
      "For example, if you have true labels stored in the variable `y_true` and predicted labels stored in the variable `y_pred`, you can calculate the confusion matrix as follows:\n",
      "\n",
      "```\n",
      "from sklearn.metrics import confusion_matrix\n",
      "confmat = confusion_matrix(y_true, y_pred)\n",
      "```\n",
      "\n",
      "The resulting `confmat` array will contain the counts of true positives, true negatives, false positives, and false negatives, which can be used to evaluate the performance of the classification model.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know how to calculate a confusion matrix.\n",
      "Final Answer: A confusion matrix is a table used to evaluate the performance of a classification model. It compares the predicted labels of the model to the actual labels of the data. To calculate a confusion matrix, you can use the `confusion_matrix` function from the `sklearn.metrics` module in Python.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "A confusion matrix is a table used to evaluate the performance of a classification model. It compares the predicted labels of the model to the actual labels of the data. To calculate a confusion matrix, you can use the `confusion_matrix` function from the `sklearn.metrics` module in Python.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Confusion Matrix and how one can caulculate it?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use QA to find the answer to this question.\n",
      "Action: QA\n",
      "Action Input: \"What is precision and recall?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPrecision and recall are performance measures used to evaluate the effectiveness of a binary classifier. \n",
      "\n",
      "Precision is the ratio of true positive predictions to the total number of positive predictions made by the classifier. It measures the accuracy of the positive predictions. A high precision indicates that the classifier has a low rate of false positives.\n",
      "\n",
      "Recall, also known as the true positive rate, is the ratio of true positive predictions to the total number of actual positive instances in the dataset. It measures the ability of the classifier to correctly identify positive instances. A high recall indicates that the classifier has a low rate of false negatives.\n",
      "\n",
      "In the context of the given information, precision and recall are used to evaluate the performance of a classifier with different thresholds. The precision/recall trade-off is discussed, highlighting the importance of considering both precision and recall when selecting a threshold for a classifier.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: Precision and recall are performance measures used to evaluate the effectiveness of a binary classifier. Precision measures the accuracy of the positive predictions, while recall measures the ability of the classifier to correctly identify positive instances.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Precision and recall are performance measures used to evaluate the effectiveness of a binary classifier. Precision measures the accuracy of the positive predictions, while recall measures the ability of the classifier to correctly identify positive instances.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is precision and recall?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to find the definition and formula for the normal equation.\n",
      "Action: QA\n",
      "Action Input: \"What is the normal equation and what is its formula?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe normal equation is a closed-form solution for finding the optimal parameters in linear regression. It allows us to directly calculate the parameters without the need for an iterative optimization algorithm like gradient descent. The formula for the normal equation is:\n",
      "\n",
      "θ = (X^T * X)^-1 * X^T * y\n",
      "\n",
      "Where:\n",
      "- θ is the vector of optimal parameters\n",
      "- X is the matrix of input features\n",
      "- y is the vector of target values\n",
      "- X^T is the transpose of X\n",
      "- (X^T * X)^-1 is the inverse of the matrix product of X^T and X\n",
      "\n",
      "By solving the normal equation, we can obtain the values of θ that minimize the cost function in linear regression.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: The normal equation is a closed-form solution for finding the optimal parameters in linear regression. Its formula is θ = (X^T * X)^-1 * X^T * y, where θ is the vector of optimal parameters, X is the matrix of input features, y is the vector of target values, X^T is the transpose of X, and (X^T * X)^-1 is the inverse of the matrix product of X^T and X.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The normal equation is a closed-form solution for finding the optimal parameters in linear regression. Its formula is θ = (X^T * X)^-1 * X^T * y, where θ is the vector of optimal parameters, X is the matrix of input features, y is the vector of target values, X^T is the transpose of X, and (X^T * X)^-1 is the inverse of the matrix product of X^T and X.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is normal equation and write its formula?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to find information on how to use SGD (Stochastic Gradient Descent) to solve a Linear Regression problem in scikit-learn.\n",
      "Action: QA\n",
      "Action Input: \"How to use SGD to solve Linear Regression problem in scikit-learn?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTo use Stochastic Gradient Descent (SGD) to solve a Linear Regression problem in scikit-learn, you can follow these steps:\n",
      "\n",
      "1. Import the necessary libraries:\n",
      "```python\n",
      "from sklearn.linear_model import SGDRegressor\n",
      "```\n",
      "\n",
      "2. Create an instance of the SGDRegressor class and set the desired parameters:\n",
      "```python\n",
      "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
      "```\n",
      "Here, `max_iter` specifies the maximum number of iterations, `tol` is the tolerance for stopping criteria, `penalty` is the regularization term (set to None for no regularization), and `eta0` is the learning rate.\n",
      "\n",
      "3. Fit the model to your training data:\n",
      "```python\n",
      "sgd_reg.fit(X, y.ravel())\n",
      "```\n",
      "Here, `X` is the feature matrix and `y` is the target variable.\n",
      "\n",
      "4. Once the model is fitted, you can access the intercept and coefficients of the linear regression equation:\n",
      "```python\n",
      "sgd_reg.intercept_, sgd_reg.coef_\n",
      "```\n",
      "The `intercept_` attribute represents the y-intercept of the linear regression line, and the `coef_` attribute represents the coefficients\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the steps to use SGD to solve a Linear Regression problem in scikit-learn.\n",
      "Final Answer: To use SGD to solve a Linear Regression problem in scikit-learn, you need to import the necessary libraries, create an instance of the SGDRegressor class with the desired parameters, fit the model to your training data, and then access the intercept and coefficients of the linear regression equation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "To use SGD to solve a Linear Regression problem in scikit-learn, you need to import the necessary libraries, create an instance of the SGDRegressor class with the desired parameters, fit the model to your training data, and then access the intercept and coefficients of the linear regression equation.\n"
     ]
    }
   ],
   "source": [
    "query = \"How one can use SGD to solve Linear Regression problem in scikit-learn?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to find information on how to use SGD to solve Logistic Regression in scikit-learn.\n",
      "Action: QA\n",
      "Action Input: \"How to use SGD to solve Logistic Regression in scikit-learn?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTo use Stochastic Gradient Descent (SGD) to solve Logistic Regression in scikit-learn, you can follow these steps:\n",
      "\n",
      "1. Import the necessary modules:\n",
      "```python\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "```\n",
      "\n",
      "2. Create an instance of the SGDClassifier class and specify the desired parameters. For example:\n",
      "```python\n",
      "sgd_clf = SGDClassifier(loss='log', max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
      "```\n",
      "Here, the 'log' loss parameter indicates that we want to perform logistic regression.\n",
      "\n",
      "3. Fit the model to the training data using the `fit()` method. For example:\n",
      "```python\n",
      "sgd_clf.fit(X_train, y_train)\n",
      "```\n",
      "Here, `X_train` represents the training features and `y_train` represents the corresponding target labels.\n",
      "\n",
      "4. Once the model is trained, you can make predictions on new data using the `predict()` method. For example:\n",
      "```python\n",
      "y_pred = sgd_clf.predict(X_test)\n",
      "```\n",
      "Here, `X_test` represents the test features and `y_pred` will contain the predicted labels for the test data.\n",
      "\n",
      "Note that the SGDClassifier in scikit-learn supports various\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found the steps to use SGD to solve Logistic Regression in scikit-learn.\n",
      "Final Answer: To use SGD to solve Logistic Regression in scikit-learn, you need to import the necessary modules, create an instance of the SGDClassifier class with the desired parameters, fit the model to the training data, and make predictions on new data using the predict() method.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "To use SGD to solve Logistic Regression in scikit-learn, you need to import the necessary modules, create an instance of the SGDClassifier class with the desired parameters, fit the model to the training data, and make predictions on new data using the predict() method.\n"
     ]
    }
   ],
   "source": [
    "query = \"How one can use SGD to solve Logsitic Regression problem in scikit-learn?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to understand what the bagging method is and how it differs from random forests.\n",
      "Action: QA\n",
      "Action Input: \"What is bagging method?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mBagging, short for bootstrap aggregating, is a method used in machine learning to improve the accuracy and reduce the variance of a model. It involves training multiple predictors (often decision trees) on different random samples of the training set. Each predictor has a higher bias than if it were trained on the original training set, but aggregation of their predictions reduces both bias and variance. The ensemble can make predictions for new instances by aggregating the predictions of all predictors using the statistical mode for classification or the average for regression. Bagging is effective in reducing the variance of a model but ineffective in reducing model bias. It is commonly used with weak learners, such as unpruned decision trees, to leverage their strengths.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now have a good understanding of the bagging method. Now I need to find out how it differs from random forests.\n",
      "Action: QA\n",
      "Action Input: \"What is the difference between bagging method and random forests?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe bagging method and random forests are both ensemble learning techniques that involve training multiple models on different subsets of the training data. However, there are some differences between the two:\n",
      "\n",
      "1. Base Estimator: In the bagging method, the base estimator is typically a decision tree. Each model in the ensemble is trained independently on a random subset of the training data. In random forests, the base estimator is also a decision tree, but with an additional randomization step. Instead of searching for the best feature when splitting a node, random forests search for the best feature among a random subset of features.\n",
      "\n",
      "2. Feature Sampling: In the bagging method, feature sampling is not performed. Each model in the ensemble is trained on a random subset of the training data, but all features are considered for splitting at each node. In random forests, feature sampling is performed in addition to instance sampling. Each model is trained on a random subset of the training data and a random subset of features. This introduces extra randomness and increases the diversity of the models in the ensemble.\n",
      "\n",
      "3. Speed: Random forests are generally faster to train compared to the bagging method. This is because random forests do not search for the best possible threshold for each feature at every node, which is a time-consuming task\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: The bagging method is a technique used in machine learning to improve accuracy and reduce variance by training multiple predictors on different random samples of the training set. It is commonly used with weak learners, such as unpruned decision trees. Random forests, on the other hand, are a variation of the bagging method that introduce additional randomization steps. In random forests, each model is trained on a random subset of the training data and a random subset of features, which increases the diversity of the models in the ensemble. Random forests are generally faster to train compared to the bagging method.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The bagging method is a technique used in machine learning to improve accuracy and reduce variance by training multiple predictors on different random samples of the training set. It is commonly used with weak learners, such as unpruned decision trees. Random forests, on the other hand, are a variation of the bagging method that introduce additional randomization steps. In random forests, each model is trained on a random subset of the training data and a random subset of features, which increases the diversity of the models in the ensemble. Random forests are generally faster to train compared to the bagging method.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is bagging method and waht is the difference between bagging method and random forests?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip-retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
